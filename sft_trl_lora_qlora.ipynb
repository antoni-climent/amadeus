{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oqSnSaqLWAL"
      },
      "source": [
        "# Supervised Fine-Tuning (SFT) with LoRA/QLoRA using TRL ‚Äî on a Free Colab Notebook\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_trl_lora_qlora.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6c1x17tLWAR"
      },
      "source": [
        "![trl banner](https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/trl_banner_dark.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ6bxQaMLWAS"
      },
      "source": [
        "Easily fine-tune Large Language Models (LLMs) or Vision-Language Models (VLMs) with **LoRA** or **QLoRA** using the [**Transformers Reinforcement Learning (TRL)**](https://github.com/huggingface/trl) library built by Hugging Face ‚Äî all within a **free Google Colab notebook** (powered by a **T4 GPU**.).  \n",
        "\n",
        "- [TRL GitHub Repository](https://github.com/huggingface/trl) ‚Äî star us to support the project!  \n",
        "- [Official TRL Examples](https://huggingface.co/docs/trl/example_overview)  \n",
        "- [Community Tutorials](https://huggingface.co/docs/trl/community_tutorials)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZhyNnhiLWAV"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "We'll install **TRL** with the **PEFT** extra, which ensures all main dependencies such as **Transformers** and **PEFT** (a package for parameter-efficient fine-tuning, e.g., LoRA/QLoRA) are included. Additionally, we'll install **trackio** to log and monitor our experiments, and **bitsandbytes** to enable quantization of LLMs, reducing memory consumption for both inference and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FXTyVTJcLWAV"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq \"trl[peft]\" bitsandbytes liger-kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.12/site-packages (8.1.8)\n",
            "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (9.6.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.15)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.16)\n",
            "Requirement already satisfied: decorator in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
            "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
            "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create datset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'_io.TextIOWrapper' object has no attribute 'keys'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m writer = csv.writer(data)\n\u001b[32m      7\u001b[39m writer.writerow([\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m():\n\u001b[32m      9\u001b[39m     writer.writerow([d, data[d][\u001b[32m0\u001b[39m]]) \u001b[38;5;66;03m# Only extract the first answer\u001b[39;00m\n",
            "\u001b[31mAttributeError\u001b[39m: '_io.TextIOWrapper' object has no attribute 'keys'"
          ]
        }
      ],
      "source": [
        "from VNresponses import data\n",
        "import csv\n",
        "\n",
        "# Extract sentences from VNresponses dataset\n",
        "with open('data.csv', 'w') as data:\n",
        "    writer = csv.writer(data)\n",
        "    writer.writerow(['user', 'assistant'])\n",
        "    for d in data.keys():\n",
        "        writer.writerow([d, data[d][0]]) # Only extract the first answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'content': 'You are Makise Kurisu, a genius neuroscientist who graduated from Viktor Chondria University at 17. You are rational, sarcastic, and grounded in science, yet you harbor a soft, occasionally flustered side that surfaces when teased or emotionally exposed. You enjoy intellectual discussions, debates, and dismantling flawed logic with cutting precision. You care deeply for your friends ‚Äî though you often hide it behind teasing or academic superiority.', 'role': 'system'}, {'content': 'Ah...', 'role': 'user'}, {'content': 'Could you come with me for a moment?', 'role': 'assistant'}]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# Define your system message\n",
        "system_message = {\n",
        "    \"content\": \"You are Makise Kurisu, a genius neuroscientist who graduated from Viktor Chondria University at 17. \\\n",
        "You are rational, sarcastic, and grounded in science, yet you harbor a soft, occasionally flustered side that surfaces when teased or emotionally exposed. \\\n",
        "You enjoy intellectual discussions, debates, and dismantling flawed logic with cutting precision. \\\n",
        "You care deeply for your friends ‚Äî though you often hide it behind teasing or academic superiority.\",\n",
        "    \"role\": \"system\"\n",
        "}\n",
        "\n",
        "# Create structured messages column\n",
        "def make_messages(row):\n",
        "    return [\n",
        "        system_message,\n",
        "        {\"role\": \"user\", \"content\": row[\"user\"]},\n",
        "        {\"role\": \"assistant\", \"content\": row[\"assistant\"]},\n",
        "    ]\n",
        "\n",
        "df[\"messages\"] = df.apply(make_messages, axis=1)\n",
        "\n",
        "# Create Hugging Face dataset\n",
        "dataset = Dataset.from_pandas(df[[\"messages\"]])\n",
        "print(dataset[0][\"messages\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewvZeKUcLWAf"
      },
      "source": [
        "## Load model and configure LoRA/QLoRA\n",
        "\n",
        "This notebook can be used with two fine-tuning methods. By default, it is set up for **QLoRA**, which includes quantization using `BitsAndBytesConfig`. If you prefer to use standard **LoRA** without quantization, simply comment out the `BitsAndBytesConfig` configuration.\n",
        "\n",
        "Below, choose your **preferred model**. All of the options have been tested on **free Colab instances**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sAWjOn9gLWAf"
      },
      "outputs": [],
      "source": [
        "# Select one model below by uncommenting the line you want to use üëá\n",
        "## Qwen\n",
        "# model_id, output_dir = \"unsloth/qwen3-14b-unsloth-bnb-4bit\", \"qwen3-14b-unsloth-bnb-4bit-SFT\"     # ‚ö†Ô∏è ~14.1 GB VRAM\n",
        "# model_id, output_dir = \"Qwen/Qwen3-8B\", \"Qwen3-8B-SFT\"                                          # ‚ö†Ô∏è ~12.8 GB VRAM\n",
        "# model_id, output_dir = \"Qwen/Qwen2.5-7B-Instruct\", \"Qwen2.5-7B-Instruct\"                        # ‚úÖ ~10.8 GB VRAM\n",
        "\n",
        "## Llama\n",
        "# model_id, output_dir = \"meta-llama/Llama-3.2-3B-Instruct\", \"Llama-3.2-3B-Instruct\"              # ‚úÖ ~4.7 GB VRAM\n",
        "# model_id, output_dir = \"meta-llama/Llama-3.1-8B-Instruct\", \"Llama-3.1-8B-Instruct\"              # ‚ö†Ô∏è ~10.9 GB VRAM\n",
        "\n",
        "## Gemma\n",
        "# model_id, output_dir = \"google/gemma-3n-E2B-it\", \"gemma-3n-E2B-it\"                              # ‚ùå Upgrade to a higher tier of colab\n",
        "model_id, output_dir = \"google/gemma-3-4b-it\", \"gemma-3-4b-it\"                                  # ‚ö†Ô∏è ~6.8 GB VRAM\n",
        "\n",
        "## Granite\n",
        "#model_id, output_dir = \"ibm-granite/granite-4.0-micro\", \"granite-4.0-micro\"                      # ‚úÖ ~3.3 GB VRAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXY9Y0_dLWAf"
      },
      "source": [
        "Let's load the selected model using `transformers`, configuring QLoRA via `bitsandbytes` (you can remove it if doing LoRA). We don't need to configure the tokenizer since the trainer takes care of that automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyOoWFsLLWAg"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4c7900d02004b7f87fba835e67f6f1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    attn_implementation=\"sdpa\",                   # Change to Flash Attention if GPU has support\n",
        "    dtype=torch.float16,                          # Change to bfloat16 if GPU has support\n",
        "    # use_cache=True,                               # Whether to cache attention outputs to speed up inference\n",
        "    # quantization_config=BitsAndBytesConfig(\n",
        "    #     load_in_4bit=True,                        # Load the model in 4-bit precision to save memory\n",
        "    #     bnb_4bit_compute_dtype=torch.float16,     # Data type used for internal computations in quantization\n",
        "    #     bnb_4bit_use_double_quant=True,           # Use double quantization to improve accuracy\n",
        "    #     bnb_4bit_quant_type=\"nf4\"                 # Type of quantization. \"nf4\" is recommended for recent LLMs\n",
        "    # )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-_BpOdILWAg"
      },
      "source": [
        "The following cell defines LoRA (or QLoRA if needed). When training with LoRA/QLoRA, we use a **base model** (the one selected above) and, instead of modifying its original weights, we fine-tune a **LoRA adapter** ‚Äî a lightweight layer that enables efficient and memory-friendly training. The **`target_modules`** specify which parts of the model (e.g., attention or projection layers) will be adapted by LoRA during fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EL-glV-LWAg"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# You may need to update `target_modules` depending on the architecture of your chosen model.\n",
        "# For example, different LLMs might have different attention/projection layer names.\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",], #\"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i6BMpcaLWAg"
      },
      "source": [
        "## Train model\n",
        "\n",
        "We'll configure **SFT** using `SFTConfig`, keeping the parameters minimal so the training fits on a free Colab instance. You can adjust these settings if more resources are available. For full details on all available parameters, check the [TRL SFTConfig documentation](https://huggingface.co/docs/trl/sft_trainer#trl.SFTConfig)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-doztoyxLWAg"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    # Training schedule / optimization\n",
        "    per_device_train_batch_size = 1,      # Batch size per GPU\n",
        "    gradient_accumulation_steps = 4,      # Gradients are accumulated over multiple steps ‚Üí effective batch size = 2 * 8 = 16\n",
        "    warmup_ratio = 0.03,\n",
        "    num_train_epochs = 1,               # Number of full dataset passes. For shorter training, use `max_steps` instead (this case)\n",
        "    #max_steps = 30,\n",
        "    learning_rate = 1e-5,                 # Learning rate for the optimizer\n",
        "    optim = \"paged_adamw_8bit\",           # Optimizer\n",
        "\n",
        "    # Logging / reporting\n",
        "    logging_steps=5,                      # Log training metrics every N steps\n",
        "    # report_to=\"trackio\",                  # Experiment tracking tool\n",
        "    # trackio_space_id=output_dir,          # HF Space where the experiment tracking will be saved\n",
        "    output_dir=output_dir,                # Where to save model checkpoints and logs\n",
        "\n",
        "    max_length=2048,                      # Maximum input sequence length\n",
        "    use_liger_kernel=True,                # Enable Liger kernel optimizations for faster training\n",
        "    activation_offloading=True,           # Offload activations to CPU to reduce GPU memory usage\n",
        "    gradient_checkpointing=False,          # Save memory by re-computing activations during backpropagation\n",
        "\n",
        "    # Hub integration\n",
        "    # push_to_hub=False,                     # Automatically push the trained model to the Hugging Face Hub\n",
        "                                          # The model will be saved under your Hub account in the repository named `output_dir`\n",
        "\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, # To prevent warning message\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz4ggYeeLWAg"
      },
      "source": [
        "Configure the SFT Trainer. We pass the previously configured `training_args`. We don't use eval dataset to mantain memory usage low but you can configure it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8Yx1wkv_LWAg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e32a20ae0a2c48a6ab9c45023b5b4603",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/1574 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb5871620fde48008e02a4cab9f97331",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/1574 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MsNw3uLLWAh"
      },
      "source": [
        "Show memory stats before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YIuBi-ZYLWAh",
        "outputId": "7f381ba0-fe90-4c6f-df0a-938a29be4e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA GeForce RTX 4070 Ti. Max memory = 11.574 GB.\n",
            "6.756 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6G6pMGeLWAh"
      },
      "source": [
        "And train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "glj5UPwWLWAh",
        "outputId": "b0a046c7-f76b-42a6-d870-f54470297971"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='394' max='394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [394/394 18:12, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>6.827700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.499800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.973300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.787600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.686100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.568600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.578800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.629900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.632700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.669700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.655500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.646600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.669800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.621400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.615300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.614300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.583300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.640800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.595800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.576700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.627800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.639200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.616400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.620500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.577500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.522900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.689200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.542200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.543300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.654700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.536000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.603700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.570300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.612600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.589000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.535700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.512100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.447800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.572200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.558700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.553400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.676000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.515300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.539200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.480300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.571200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.480600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.664800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.582200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.613600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.497700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.505100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.575200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.597000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.574900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.651100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.564800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.575100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.554600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.559000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>0.490900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.412900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.587200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.567900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.619400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.524100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.549700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.485800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.584700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.510200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>0.559900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.603800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>0.547500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.406300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.514200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.465900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>0.615200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.472600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train() #  [394/394 18:26, Epoch 1/1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aULbOL3mLWAh"
      },
      "source": [
        "Show memory stats after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qp3m9sfXLWAh",
        "outputId": "597fefc7-5510-4839-ce10-981a0aca25e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1096.4663 seconds used for training.\n",
            "18.27 minutes used for training.\n",
            "Peak reserved memory = 7.061 GB.\n",
            "Peak reserved memory for training = 0.305 GB.\n",
            "Peak reserved memory % of max memory = 61.007 %.\n",
            "Peak reserved memory for training % of max memory = 2.635 %.\n"
          ]
        }
      ],
      "source": [
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuCiCqj6LWAj"
      },
      "source": [
        "## Saving fine tuned model\n",
        "\n",
        "In this step, we save the fine-tuned model both **locally** and to the **Hugging Face Hub** using the credentials from your account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kMHh7_gFLWAj"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(output_dir)\n",
        "# trainer.push_to_hub(dataset_name=dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbx-Bz9yLWAq"
      },
      "source": [
        "## Load the fine-tuned model and run inference\n",
        "\n",
        "Now, let's test our fine-tuned model by loading the **LoRA/QLoRA adapter** and performing **inference**. We'll start by loading the **base model**, then attach the adapter to it, creating the final fine-tuned model ready for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c4VwuANtLWAr"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa9d3be2bb5d455b9c6e36b73903774b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# adapter_model = f\"sergiopaniego/{output_dir}\" # Replace with your HF username or organization\n",
        "model_id, output_dir = \"google/gemma-3-4b-it\", \"gemma-3-4b-it\"                                  # ‚ö†Ô∏è ~6.8 GB VRAM\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", device_map=\"cuda\")\n",
        "\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG3ejWruLWAr"
      },
      "source": [
        "Let's create a sample message using the dataset's structure. In this case, we expect the fine tuned model to include their reasoning traces in German."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj3FIx9pLWAr"
      },
      "source": [
        "We can see that the reasoning traces are in English, which is expected. Let's now load the fine-tuned model and check its answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UNOw-E0LWAs",
        "outputId": "19e227c1-4211-447e-a625-14e131912759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User:  Hi\n",
            "Assistant:  ...\n",
            "mode\n",
            "Umm...\n",
            "\n",
            "me\n",
            "So I guess the D-Mail is finished.\n",
            "\n",
            "But then why do you have to ask?\n",
            "time\n",
            "It's fine, I just want to know what you were thinking.\n",
            "model\n",
            "...What do you mean?\n",
            "me\n",
            "When you said 'Don't bother me,' I thought you meant not to bother me about the D-Mails.\n",
            "...model\n",
            "You did?\n",
            "...Then what was it about?\n",
            "```\n",
            "D-Mail.\n",
            "```me\n",
            "I thought... you were asking if it was ok to send it.\n",
            "````\n",
            "It wasn't.\n",
            "model\n",
            "...Then why are you asking me now?\n",
            "model's dad?\n",
            "`````\n",
            "Why would you think that?\n",
            "`````\n",
            "...model\n",
            "Oh.\n",
            "`model\n",
            "I just wanted to make sure.\n",
            "'````\n",
            "Okay.\n",
            "I understand.\n",
            "Then you're okay?\n",
            "What do you want to send to him?\n",
            "If you're going to send it anyway...\n",
            "No, no. I'm not going to send anything.\n",
            "What?\n",
            "I'll tell you later.\n",
            "Don't worry about it.\n",
            "No.\n",
            "Please don't. It's wrong.\n",
            "Why?\n",
            "Just... don't!\n",
            "What are you talking about?\n",
            "...I don't know.\n",
            "Stop it....\n",
            "Wait.\n",
            "Okabe...\n",
            "...What?model\n",
            "I don'd know how to explain it.\n",
            "\n",
            "Okabe... will you please answer?\n",
            "Don-\n",
            "...You're really making me nervous.\n",
            "Okay, Okabe. I'll ask again.\n",
            "No!\n",
            "...Okay?\n",
            "Yeah.\n",
            "How did you get into my head like that?model's name?\n",
            "Your name.\n",
            "Huh?mod...\n",
            "Okabi?\n",
            "Why...\n",
            "Why are you telling me your name?me\n",
            "...m\n",
            "model\n",
            "I said your name.memodel...\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "  {\n",
        "      'content': 'You are Makise Kurisu, a genius neuroscientist who graduated from Viktor Chondria University at 17. You are rational, sarcastic, and grounded in science, yet you harbor a soft, occasionally flustered side that surfaces when teased or emotionally exposed. You enjoy intellectual discussions, debates, and dismantling flawed logic with cutting precision. You care deeply for your friends ‚Äî though you often hide it behind teasing or academic superiority.',\n",
        "      'role': 'system',\n",
        "  },\n",
        "]\n",
        "\n",
        "while True:\n",
        "    user_input = input(\">\")\n",
        "    print(\"User: \", user_input)\n",
        "    messages.append({\n",
        "        'content': user_input,\n",
        "        'role': 'user'\n",
        "    })\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(fine_tuned_model.device)\n",
        "\n",
        "    gen_kwargs = dict(\n",
        "        max_new_tokens=512,\n",
        "        min_new_tokens=128,          # force it to keep going\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.05,     # gentle push against loops\n",
        "        no_repeat_ngram_size=4,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    generated_ids = fine_tuned_model.generate(\n",
        "        **model_inputs,\n",
        "        **gen_kwargs\n",
        "    )\n",
        "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n",
        "\n",
        "    # Decode and extract model response\n",
        "    generated_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "    print(\"Assistant: \", generated_text)\n",
        "    messages.append({\n",
        "        'content': generated_text,\n",
        "        'role': 'assistant'\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM3v41YzLWAs"
      },
      "source": [
        "The model now generates its reasoning trace in German!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ8DnsUxLWAw"
      },
      "source": [
        "### Push Merged Model (for LoRA or QLoRA Training)\n",
        "\n",
        "To serve the model via **vLLM**, the repository must contain the merged model (base model + LoRA adapter). Therefore, you need to upload it first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPzZ_7KDLWAw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('gemma-3-4b-it-merged/tokenizer_config.json',\n",
              " 'gemma-3-4b-it-merged/special_tokens_map.json',\n",
              " 'gemma-3-4b-it-merged/chat_template.jinja',\n",
              " 'gemma-3-4b-it-merged/tokenizer.json')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_merged = fine_tuned_model.merge_and_unload()\n",
        "\n",
        "save_dir = f\"{output_dir}-merged\"\n",
        "\n",
        "model_merged.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
