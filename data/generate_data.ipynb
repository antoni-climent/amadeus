{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52934a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# adapter_model = f\"sergiopaniego/{output_dir}\" # Replace with your HF username or organization\n",
    "model_id, output_dir = \"google/gemma-3-4b-it\", \"../gemma-3-4b-it\"                                  # ⚠️ ~6.8 GB VRAM\n",
    "\n",
    "model_id = \"microsoft/phi-4\"\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", device_map=\"cuda\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    attn_implementation=\"sdpa\",                   # Change to Flash Attention if GPU has support\n",
    "    dtype='auto',                          # Change to bfloat16 if GPU has support\n",
    "    device_map='cuda',\n",
    "    # use_cache=True,                               # Whether to cache attention outputs to speed up inference\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                        # Load the model in 4-bit precision to save memory\n",
    "        bnb_4bit_compute_dtype=torch.float16,     # Data type used for internal computations in quantization\n",
    "        bnb_4bit_use_double_quant=True,           # Use double quantization to improve accuracy\n",
    "        bnb_4bit_quant_type=\"nf4\"                 # Type of quantization. \"nf4\" is recommended for recent LLMs\n",
    "    )\n",
    ")\n",
    "# fine_tuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84254468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(fine_tuned_model, tokenizer, messages):\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\", add_special_tokens=False).to(fine_tuned_model.device)\n",
    "\n",
    "    eos = tokenizer.eos_token_id\n",
    "    eot = tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "    terminators = [i for i in [eos, eot] if i is not None]\n",
    "\n",
    "    generated_ids = fine_tuned_model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True, temperature=0.2, top_p=0.9,\n",
    "        repetition_penalty=1.05, no_repeat_ngram_size=4,\n",
    "        # eos_token_id=terminators,  # stop on EOS or EOT\n",
    "        # pad_token_id=tokenizer.pad_token_id or eos,\n",
    "    )\n",
    "    # print(generated_ids[0])\n",
    "    # print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "    # print(\"-----------\")\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n",
    "\n",
    "    # Decode and extract model response\n",
    "    generated_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fcce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "messages = [\n",
    "  {\n",
    "      'content': 'Given a line of dialogue from the user, generate a single, natural-sounding line that someone else could have said immediately before it in a realistic conversation. \\\n",
    "The response should feel contextually appropriate, emotionally coherent, and distinct in voice from the user’s line, as if two different people were talking. \\\n",
    "Output only the preceding line, with no quotes, explanations, or additional text.',\n",
    "      'role': 'system',\n",
    "  },\n",
    "]\n",
    "\n",
    "with open('VNKurisuDialogues.csv', 'r') as file:\n",
    "    with open('synthetic_data.csv', 'w', newline='') as output_file:\n",
    "        writer = csv.writer(output_file)\n",
    "        writer.writerow(['user', 'assistant'])  # Write header row\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            messages.append({\n",
    "                'content': row[1],\n",
    "                'role': 'user'\n",
    "            })\n",
    "            generated_text = inference(base_model, tokenizer, messages)\n",
    "            writer.writerow([generated_text, row[1]])  # Write the user input and generated text to the CSV\n",
    "            # Flush so that data is written incrementally\n",
    "            output_file.flush()\n",
    "            # Remove the last user message to avoid context buildup\n",
    "            messages.pop()\n",
    "            \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
