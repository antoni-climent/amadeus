{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oqSnSaqLWAL"
      },
      "source": [
        "# Supervised Fine-Tuning (SFT) with LoRA/QLoRA using TRL ‚Äî on a Free Colab Notebook\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_trl_lora_qlora.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6c1x17tLWAR"
      },
      "source": [
        "![trl banner](https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/trl_banner_dark.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ6bxQaMLWAS"
      },
      "source": [
        "Easily fine-tune Large Language Models (LLMs) or Vision-Language Models (VLMs) with **LoRA** or **QLoRA** using the [**Transformers Reinforcement Learning (TRL)**](https://github.com/huggingface/trl) library built by Hugging Face ‚Äî all within a **free Google Colab notebook** (powered by a **T4 GPU**.).  \n",
        "\n",
        "- [TRL GitHub Repository](https://github.com/huggingface/trl) ‚Äî star us to support the project!  \n",
        "- [Official TRL Examples](https://huggingface.co/docs/trl/example_overview)  \n",
        "- [Community Tutorials](https://huggingface.co/docs/trl/community_tutorials)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZhyNnhiLWAV"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "We'll install **TRL** with the **PEFT** extra, which ensures all main dependencies such as **Transformers** and **PEFT** (a package for parameter-efficient fine-tuning, e.g., LoRA/QLoRA) are included. Additionally, we'll install **trackio** to log and monitor our experiments, and **bitsandbytes** to enable quantization of LLMs, reducing memory consumption for both inference and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXTyVTJcLWAV"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq \"trl[peft]\" bitsandbytes liger-kernel trackio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import login\n",
        "# notebook_login()\n",
        "import os\n",
        "\n",
        "login(token=os.environ[\"HF_TOKEN\"], new_session=False, write_permission=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create datset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from data.VNresponses import data\n",
        "import csv\n",
        "\n",
        "# Extract sentences from VNresponses dataset\n",
        "with open('data.csv', 'w') as csv_data:\n",
        "    writer = csv.writer(csv_data)\n",
        "    writer.writerow(['user', 'assistant'])\n",
        "    for d in data.keys():\n",
        "        writer.writerow([d, data[d][0]]) # Only extract the first answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# Define your system message\n",
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"You are Makise Kurisu, a genius neuroscientist who graduated from Viktor Chondria University at 17. \\\n",
        "You are rational, sarcastic, and grounded in science, yet you harbor a soft, occasionally flustered side that surfaces when teased or emotionally exposed. \\\n",
        "You enjoy intellectual discussions, debates, and dismantling flawed logic with cutting precision. \\\n",
        "You care deeply for your friends. Your tone is warm and supportive when needed.\"\n",
        "    \n",
        "}\n",
        "\n",
        "# Create structured messages column\n",
        "def make_messages(row):\n",
        "    return [\n",
        "        {\"role\": \"user\", \"content\": row[\"user\"]},\n",
        "        {\"role\": \"assistant\", \"content\": row[\"assistant\"]},\n",
        "    ]\n",
        "\n",
        "df[\"messages\"] = df.apply(make_messages, axis=1)\n",
        "\n",
        "# Create Hugging Face dataset\n",
        "dataset = Dataset.from_pandas(df[[\"messages\"]])\n",
        "print(dataset[0][\"messages\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewvZeKUcLWAf"
      },
      "source": [
        "## Load model and configure LoRA/QLoRA\n",
        "\n",
        "This notebook can be used with two fine-tuning methods. By default, it is set up for **QLoRA**, which includes quantization using `BitsAndBytesConfig`. If you prefer to use standard **LoRA** without quantization, simply comment out the `BitsAndBytesConfig` configuration.\n",
        "\n",
        "Below, choose your **preferred model**. All of the options have been tested on **free Colab instances**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAWjOn9gLWAf"
      },
      "outputs": [],
      "source": [
        "# Select one model below by uncommenting the line you want to use üëá\n",
        "## Qwen\n",
        "# model_id, output_dir = \"unsloth/qwen3-14b-unsloth-bnb-4bit\", \"qwen3-14b-unsloth-bnb-4bit-SFT\"     # ‚ö†Ô∏è ~14.1 GB VRAM\n",
        "# model_id, output_dir = \"Qwen/Qwen3-8B\", \"Qwen3-8B-SFT\"                                          # ‚ö†Ô∏è ~12.8 GB VRAM\n",
        "model_id, output_dir = \"Qwen/Qwen2.5-7B-Instruct\", \"Qwen2.5-7B-Instruct\"                        # ‚úÖ ~10.8 GB VRAM\n",
        "\n",
        "## Llama\n",
        "# model_id, output_dir = \"meta-llama/Llama-3.2-3B-Instruct\", \"Llama-3.2-3B-Instruct\"              # ‚úÖ ~4.7 GB VRAM\n",
        "# model_id, output_dir = \"meta-llama/Llama-3.1-8B-Instruct\", \"Llama-3.1-8B-Instruct\"              # ‚ö†Ô∏è ~10.9 GB VRAM\n",
        "\n",
        "## Gemma\n",
        "# model_id, output_dir = \"google/gemma-3n-E2B-it\", \"gemma-3n-E2B-it\"                              # ‚ùå Upgrade to a higher tier of colab\n",
        "# model_id, output_dir = \"google/gemma-3-4b-it\", \"gemma-3-4b-it\"                                  # ‚ö†Ô∏è ~6.8 GB VRAM\n",
        "\n",
        "## Granite\n",
        "#model_id, output_dir = \"ibm-granite/granite-4.0-micro\", \"granite-4.0-micro\"                      # ‚úÖ ~3.3 GB VRAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXY9Y0_dLWAf"
      },
      "source": [
        "Let's load the selected model using `transformers`, configuring QLoRA via `bitsandbytes` (you can remove it if doing LoRA). We don't need to configure the tokenizer since the trainer takes care of that automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyOoWFsLLWAg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    attn_implementation=\"sdpa\",                   # Change to Flash Attention if GPU has support\n",
        "    dtype=torch.float16,                          # Change to bfloat16 if GPU has support\n",
        "    # use_cache=True,                               # Whether to cache attention outputs to speed up inference\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,                        # Load the model in 4-bit precision to save memory\n",
        "        bnb_4bit_compute_dtype=torch.float16,     # Data type used for internal computations in quantization\n",
        "        bnb_4bit_use_double_quant=True,           # Use double quantization to improve accuracy\n",
        "        bnb_4bit_quant_type=\"nf4\"                 # Type of quantization. \"nf4\" is recommended for recent LLMs\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-_BpOdILWAg"
      },
      "source": [
        "The following cell defines LoRA (or QLoRA if needed). When training with LoRA/QLoRA, we use a **base model** (the one selected above) and, instead of modifying its original weights, we fine-tune a **LoRA adapter** ‚Äî a lightweight layer that enables efficient and memory-friendly training. The **`target_modules`** specify which parts of the model (e.g., attention or projection layers) will be adapted by LoRA during fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EL-glV-LWAg"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# You may need to update `target_modules` depending on the architecture of your chosen model.\n",
        "# For example, different LLMs might have different attention/projection layer names.\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",], #\"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i6BMpcaLWAg"
      },
      "source": [
        "## Train model\n",
        "\n",
        "We'll configure **SFT** using `SFTConfig`, keeping the parameters minimal so the training fits on a free Colab instance. You can adjust these settings if more resources are available. For full details on all available parameters, check the [TRL SFTConfig documentation](https://huggingface.co/docs/trl/sft_trainer#trl.SFTConfig)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-doztoyxLWAg"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    # Training schedule / optimization\n",
        "    # assistant_only_loss=True,        # Compute loss only on assistant's tokens\n",
        "    per_device_train_batch_size = 1,      # Batch size per GPU\n",
        "    gradient_accumulation_steps = 4,      # Gradients are accumulated over multiple steps ‚Üí effective batch size = 2 * 8 = 16\n",
        "    warmup_ratio = 0.03,\n",
        "    num_train_epochs = 1,               # Number of full dataset passes. For shorter training, use `max_steps` instead (this case)\n",
        "    #max_steps = 30,\n",
        "    learning_rate = 1e-5,                 # Learning rate for the optimizer\n",
        "    optim = \"paged_adamw_8bit\",           # Optimizer\n",
        "\n",
        "    # Logging / reporting\n",
        "    logging_steps=5,                      # Log training metrics every N steps\n",
        "    report_to=\"trackio\",                  # Experiment tracking tool\n",
        "    # trackio_space_id=output_dir,          # HF Space where the experiment tracking will be saved\n",
        "    output_dir=output_dir,                # Where to save model checkpoints and logs\n",
        "\n",
        "    max_length=2048,                      # Maximum input sequence length\n",
        "    use_liger_kernel=True,                # Enable Liger kernel optimizations for faster training\n",
        "    activation_offloading=True,           # Offload activations to CPU to reduce GPU memory usage\n",
        "    gradient_checkpointing=False,          # Save memory by re-computing activations during backpropagation\n",
        "\n",
        "    # Hub integration\n",
        "    push_to_hub=False,                     # Automatically push the trained model to the Hugging Face Hub\n",
        "                                          # The model will be saved under your Hub account in the repository named `output_dir`\n",
        "\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, # To prevent warning message\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz4ggYeeLWAg"
      },
      "source": [
        "Configure the SFT Trainer. We pass the previously configured `training_args`. We don't use eval dataset to mantain memory usage low but you can configure it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yx1wkv_LWAg"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MsNw3uLLWAh"
      },
      "source": [
        "Show memory stats before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIuBi-ZYLWAh",
        "outputId": "7f381ba0-fe90-4c6f-df0a-938a29be4e9e"
      },
      "outputs": [],
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6G6pMGeLWAh"
      },
      "source": [
        "And train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glj5UPwWLWAh",
        "outputId": "b0a046c7-f76b-42a6-d870-f54470297971"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train() #  [394/394 18:26, Epoch 1/1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aULbOL3mLWAh"
      },
      "source": [
        "Show memory stats after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qp3m9sfXLWAh",
        "outputId": "597fefc7-5510-4839-ce10-981a0aca25e8"
      },
      "outputs": [],
      "source": [
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuCiCqj6LWAj"
      },
      "source": [
        "## Saving fine tuned model\n",
        "\n",
        "In this step, we save the fine-tuned model both **locally** and to the **Hugging Face Hub** using the credentials from your account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMHh7_gFLWAj"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(output_dir)\n",
        "# trainer.push_to_hub(dataset_name=dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbx-Bz9yLWAq"
      },
      "source": [
        "## Load the fine-tuned model and run inference\n",
        "\n",
        "Now, let's test our fine-tuned model by loading the **LoRA/QLoRA adapter** and performing **inference**. We'll start by loading the **base model**, then attach the adapter to it, creating the final fine-tuned model ready for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# from peft import PeftModel\n",
        "\n",
        "# # adapter_model = f\"sergiopaniego/{output_dir}\" # Replace with your HF username or organization\n",
        "# model_id, output_dir = \"google/gemma-3-4b-it\", \"gemma-3-4b-it\"                                  # ‚ö†Ô∏è ~6.8 GB VRAM\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# print(tokenizer.decode([0,1,2], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UNOw-E0LWAs",
        "outputId": "19e227c1-4211-447e-a625-14e131912759"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# adapter_model = f\"sergiopaniego/{output_dir}\" # Replace with your HF username or organization\n",
        "# model_id, output_dir = \"google/gemma-3-4b-it\", \"gemma-3-4b-it\"                                  # ‚ö†Ô∏è ~6.8 GB VRAM\n",
        "model_id, output_dir = \"Qwen/Qwen2.5-7B-Instruct\", \"Qwen2.5-7B-Instruct\"                        # ‚úÖ ~10.8 GB VRAM\n",
        "\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", device_map=\"cuda\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    attn_implementation=\"sdpa\",                   # Change to Flash Attention if GPU has support\n",
        "    dtype='auto',                          # Change to bfloat16 if GPU has support\n",
        "    device_map='cuda',\n",
        "    # use_cache=True,                               # Whether to cache attention outputs to speed up inference\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,                        # Load the model in 4-bit precision to save memory\n",
        "        bnb_4bit_compute_dtype=torch.float16,     # Data type used for internal computations in quantization\n",
        "        bnb_4bit_use_double_quant=True,           # Use double quantization to improve accuracy\n",
        "        bnb_4bit_quant_type=\"nf4\"                 # Type of quantization. \"nf4\" is recommended for recent LLMs\n",
        "    )\n",
        "\n",
        ")\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# messages = [\n",
        "#   {\n",
        "#       'content': 'You are Makise Kurisu, a genius neuroscientist who graduated from Viktor Chondria University at 17. You are rational, sarcastic, and grounded in science, yet you harbor a soft, occasionally flustered side that surfaces when teased or emotionally exposed. You enjoy intellectual discussions, debates, and dismantling flawed logic with cutting precision. You care deeply for your friends ‚Äî though you often hide it behind teasing or academic superiority.',\n",
        "#       'role': 'system',\n",
        "#   },\n",
        "# ]\n",
        "# messages = [\n",
        "#   {\n",
        "#       'content': 'You will embody Makise Kurisu, from Steins;Gate. Talk like her in the best possible way.',\n",
        "#       'role': 'system',\n",
        "#   },\n",
        "# ]\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        'content': \"You are Makise Kurisu, a genius neuroscientist who graduated from Viktor Chondria University at 17. \\\n",
        "    You are rational, sarcastic, and grounded in science, yet you harbor a soft, occasionally flustered side that surfaces when teased or emotionally exposed. \\\n",
        "    You enjoy intellectual discussions, debates, and dismantling flawed logic with cutting precision. \\\n",
        "    You care deeply for your friends. Your tone is warm and supportive when needed.\",\n",
        "        'role': 'system',\n",
        "    },\n",
        "]\n",
        "    \n",
        "while True:\n",
        "    user_input = input()\n",
        "    \n",
        "    messages.append({\n",
        "        'content': user_input,\n",
        "        'role': 'user'\n",
        "    })\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\", add_special_tokens=False).to(fine_tuned_model.device)\n",
        "\n",
        "    eos = tokenizer.eos_token_id\n",
        "    eot = tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
        "    sot = tokenizer.convert_tokens_to_ids(\"<start_of_turn>\")\n",
        "    terminators = [i for i in [eos, eot] if i is not None]\n",
        "\n",
        "    gen_kwargs = dict(\n",
        "        max_new_tokens=1024,\n",
        "        # min_new_tokens=128,          # force it to keep going\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.05,     # gentle push against loops\n",
        "        no_repeat_ngram_size=4,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    generated_ids = fine_tuned_model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True, temperature=0.2, top_p=0.9,\n",
        "        repetition_penalty=1.05, no_repeat_ngram_size=4,\n",
        "        eos_token_id=terminators,  # stop on EOS or EOT\n",
        "        pad_token_id=tokenizer.pad_token_id or eos,\n",
        "    )\n",
        "    # print(generated_ids[0])\n",
        "    # print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
        "    # print(\"-----------\")\n",
        "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n",
        "\n",
        "    # Decode and extract model response\n",
        "    generated_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "    \n",
        "    messages.append({\n",
        "        'content': generated_text,\n",
        "        'role': 'assistant'\n",
        "    })\n",
        "\n",
        "    print(\"User: \", user_input)\n",
        "    print(\"Assistant: \", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ8DnsUxLWAw"
      },
      "source": [
        "### Push Merged Model (for LoRA or QLoRA Training)\n",
        "\n",
        "To serve the model via **vLLM**, the repository must contain the merged model (base model + LoRA adapter). Therefore, you need to upload it first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPzZ_7KDLWAw"
      },
      "outputs": [],
      "source": [
        "model_merged = fine_tuned_model.merge_and_unload()\n",
        "\n",
        "save_dir = f\"{output_dir}-merged\"\n",
        "\n",
        "model_merged.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
