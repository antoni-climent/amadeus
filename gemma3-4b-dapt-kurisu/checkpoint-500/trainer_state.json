{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.35285815102328866,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0035285815102328866,
      "grad_norm": 25.921674728393555,
      "learning_rate": 9.30232558139535e-07,
      "loss": 10.2292,
      "num_tokens": 144.0,
      "step": 5
    },
    {
      "epoch": 0.007057163020465773,
      "grad_norm": 27.887800216674805,
      "learning_rate": 2.0930232558139536e-06,
      "loss": 8.5953,
      "num_tokens": 304.0,
      "step": 10
    },
    {
      "epoch": 0.01058574453069866,
      "grad_norm": 25.759153366088867,
      "learning_rate": 3.2558139534883724e-06,
      "loss": 9.1102,
      "num_tokens": 453.0,
      "step": 15
    },
    {
      "epoch": 0.014114326040931546,
      "grad_norm": 21.76809310913086,
      "learning_rate": 4.418604651162791e-06,
      "loss": 11.0589,
      "num_tokens": 577.0,
      "step": 20
    },
    {
      "epoch": 0.017642907551164433,
      "grad_norm": 17.176002502441406,
      "learning_rate": 5.58139534883721e-06,
      "loss": 8.1383,
      "num_tokens": 722.0,
      "step": 25
    },
    {
      "epoch": 0.02117148906139732,
      "grad_norm": 10.399138450622559,
      "learning_rate": 6.744186046511628e-06,
      "loss": 9.2162,
      "num_tokens": 861.0,
      "step": 30
    },
    {
      "epoch": 0.024700070571630206,
      "grad_norm": 13.782427787780762,
      "learning_rate": 7.906976744186048e-06,
      "loss": 8.3233,
      "num_tokens": 1027.0,
      "step": 35
    },
    {
      "epoch": 0.028228652081863093,
      "grad_norm": 12.461374282836914,
      "learning_rate": 9.069767441860465e-06,
      "loss": 7.7279,
      "num_tokens": 1191.0,
      "step": 40
    },
    {
      "epoch": 0.03175723359209598,
      "grad_norm": 4.2789530754089355,
      "learning_rate": 9.992721979621543e-06,
      "loss": 6.8317,
      "num_tokens": 1420.0,
      "step": 45
    },
    {
      "epoch": 0.035285815102328866,
      "grad_norm": 5.528181552886963,
      "learning_rate": 9.956331877729258e-06,
      "loss": 7.3611,
      "num_tokens": 1614.0,
      "step": 50
    },
    {
      "epoch": 0.03881439661256175,
      "grad_norm": 4.301918029785156,
      "learning_rate": 9.919941775836974e-06,
      "loss": 5.8643,
      "num_tokens": 1854.0,
      "step": 55
    },
    {
      "epoch": 0.04234297812279464,
      "grad_norm": 5.580268383026123,
      "learning_rate": 9.883551673944687e-06,
      "loss": 5.9467,
      "num_tokens": 2041.0,
      "step": 60
    },
    {
      "epoch": 0.045871559633027525,
      "grad_norm": 3.7024829387664795,
      "learning_rate": 9.847161572052403e-06,
      "loss": 7.7714,
      "num_tokens": 2217.0,
      "step": 65
    },
    {
      "epoch": 0.04940014114326041,
      "grad_norm": 8.496533393859863,
      "learning_rate": 9.810771470160118e-06,
      "loss": 6.629,
      "num_tokens": 2405.0,
      "step": 70
    },
    {
      "epoch": 0.0529287226534933,
      "grad_norm": 4.015966415405273,
      "learning_rate": 9.774381368267832e-06,
      "loss": 6.2269,
      "num_tokens": 2575.0,
      "step": 75
    },
    {
      "epoch": 0.056457304163726185,
      "grad_norm": 6.788255214691162,
      "learning_rate": 9.737991266375547e-06,
      "loss": 6.3991,
      "num_tokens": 2760.0,
      "step": 80
    },
    {
      "epoch": 0.05998588567395907,
      "grad_norm": 8.681844711303711,
      "learning_rate": 9.701601164483261e-06,
      "loss": 7.2611,
      "num_tokens": 2869.0,
      "step": 85
    },
    {
      "epoch": 0.06351446718419196,
      "grad_norm": 9.664346694946289,
      "learning_rate": 9.665211062590975e-06,
      "loss": 5.609,
      "num_tokens": 3039.0,
      "step": 90
    },
    {
      "epoch": 0.06704304869442484,
      "grad_norm": 5.559340953826904,
      "learning_rate": 9.62882096069869e-06,
      "loss": 6.7479,
      "num_tokens": 3198.0,
      "step": 95
    },
    {
      "epoch": 0.07057163020465773,
      "grad_norm": 13.2333402633667,
      "learning_rate": 9.592430858806406e-06,
      "loss": 5.5961,
      "num_tokens": 3376.0,
      "step": 100
    },
    {
      "epoch": 0.07410021171489062,
      "grad_norm": 6.381357192993164,
      "learning_rate": 9.55604075691412e-06,
      "loss": 4.8746,
      "num_tokens": 3511.0,
      "step": 105
    },
    {
      "epoch": 0.0776287932251235,
      "grad_norm": 7.3606109619140625,
      "learning_rate": 9.519650655021835e-06,
      "loss": 6.1467,
      "num_tokens": 3647.0,
      "step": 110
    },
    {
      "epoch": 0.08115737473535639,
      "grad_norm": 6.588961124420166,
      "learning_rate": 9.48326055312955e-06,
      "loss": 5.6679,
      "num_tokens": 3813.0,
      "step": 115
    },
    {
      "epoch": 0.08468595624558928,
      "grad_norm": 6.097724914550781,
      "learning_rate": 9.446870451237264e-06,
      "loss": 5.4035,
      "num_tokens": 3963.0,
      "step": 120
    },
    {
      "epoch": 0.08821453775582216,
      "grad_norm": 8.375110626220703,
      "learning_rate": 9.41048034934498e-06,
      "loss": 5.2554,
      "num_tokens": 4102.0,
      "step": 125
    },
    {
      "epoch": 0.09174311926605505,
      "grad_norm": 5.556113243103027,
      "learning_rate": 9.374090247452695e-06,
      "loss": 6.3747,
      "num_tokens": 4244.0,
      "step": 130
    },
    {
      "epoch": 0.09527170077628794,
      "grad_norm": 3.689164400100708,
      "learning_rate": 9.337700145560408e-06,
      "loss": 4.6173,
      "num_tokens": 4426.0,
      "step": 135
    },
    {
      "epoch": 0.09880028228652082,
      "grad_norm": 9.203723907470703,
      "learning_rate": 9.301310043668122e-06,
      "loss": 4.5421,
      "num_tokens": 4627.0,
      "step": 140
    },
    {
      "epoch": 0.10232886379675371,
      "grad_norm": 7.6088972091674805,
      "learning_rate": 9.264919941775838e-06,
      "loss": 5.4274,
      "num_tokens": 4742.0,
      "step": 145
    },
    {
      "epoch": 0.1058574453069866,
      "grad_norm": 7.335166931152344,
      "learning_rate": 9.228529839883553e-06,
      "loss": 4.8092,
      "num_tokens": 4886.0,
      "step": 150
    },
    {
      "epoch": 0.10938602681721948,
      "grad_norm": 11.899389266967773,
      "learning_rate": 9.192139737991267e-06,
      "loss": 5.5594,
      "num_tokens": 5021.0,
      "step": 155
    },
    {
      "epoch": 0.11291460832745237,
      "grad_norm": 10.671916961669922,
      "learning_rate": 9.155749636098982e-06,
      "loss": 4.9508,
      "num_tokens": 5160.0,
      "step": 160
    },
    {
      "epoch": 0.11644318983768526,
      "grad_norm": 5.434268951416016,
      "learning_rate": 9.119359534206696e-06,
      "loss": 4.714,
      "num_tokens": 5308.0,
      "step": 165
    },
    {
      "epoch": 0.11997177134791814,
      "grad_norm": 4.767143726348877,
      "learning_rate": 9.082969432314411e-06,
      "loss": 4.222,
      "num_tokens": 5490.0,
      "step": 170
    },
    {
      "epoch": 0.12350035285815103,
      "grad_norm": 11.47334098815918,
      "learning_rate": 9.046579330422127e-06,
      "loss": 4.7697,
      "num_tokens": 5675.0,
      "step": 175
    },
    {
      "epoch": 0.12702893436838392,
      "grad_norm": 5.585015773773193,
      "learning_rate": 9.01018922852984e-06,
      "loss": 4.571,
      "num_tokens": 5853.0,
      "step": 180
    },
    {
      "epoch": 0.1305575158786168,
      "grad_norm": 6.937180995941162,
      "learning_rate": 8.973799126637556e-06,
      "loss": 4.2559,
      "num_tokens": 6022.0,
      "step": 185
    },
    {
      "epoch": 0.1340860973888497,
      "grad_norm": 7.042027950286865,
      "learning_rate": 8.937409024745271e-06,
      "loss": 3.9259,
      "num_tokens": 6176.0,
      "step": 190
    },
    {
      "epoch": 0.13761467889908258,
      "grad_norm": 12.704859733581543,
      "learning_rate": 8.901018922852985e-06,
      "loss": 4.4998,
      "num_tokens": 6340.0,
      "step": 195
    },
    {
      "epoch": 0.14114326040931546,
      "grad_norm": 7.44657564163208,
      "learning_rate": 8.864628820960699e-06,
      "loss": 3.7444,
      "num_tokens": 6502.0,
      "step": 200
    },
    {
      "epoch": 0.14467184191954835,
      "grad_norm": 7.578185081481934,
      "learning_rate": 8.828238719068414e-06,
      "loss": 4.322,
      "num_tokens": 6695.0,
      "step": 205
    },
    {
      "epoch": 0.14820042342978124,
      "grad_norm": 11.32441520690918,
      "learning_rate": 8.79184861717613e-06,
      "loss": 5.0231,
      "num_tokens": 6796.0,
      "step": 210
    },
    {
      "epoch": 0.15172900494001412,
      "grad_norm": 9.793000221252441,
      "learning_rate": 8.755458515283843e-06,
      "loss": 3.9224,
      "num_tokens": 6954.0,
      "step": 215
    },
    {
      "epoch": 0.155257586450247,
      "grad_norm": 9.136043548583984,
      "learning_rate": 8.719068413391559e-06,
      "loss": 4.3296,
      "num_tokens": 7125.0,
      "step": 220
    },
    {
      "epoch": 0.1587861679604799,
      "grad_norm": 8.767952919006348,
      "learning_rate": 8.682678311499272e-06,
      "loss": 4.8129,
      "num_tokens": 7294.0,
      "step": 225
    },
    {
      "epoch": 0.16231474947071278,
      "grad_norm": 9.559598922729492,
      "learning_rate": 8.646288209606988e-06,
      "loss": 4.3172,
      "num_tokens": 7468.0,
      "step": 230
    },
    {
      "epoch": 0.16584333098094567,
      "grad_norm": 10.590702056884766,
      "learning_rate": 8.609898107714703e-06,
      "loss": 3.7625,
      "num_tokens": 7654.0,
      "step": 235
    },
    {
      "epoch": 0.16937191249117856,
      "grad_norm": 7.263049602508545,
      "learning_rate": 8.573508005822417e-06,
      "loss": 4.6005,
      "num_tokens": 7842.0,
      "step": 240
    },
    {
      "epoch": 0.17290049400141144,
      "grad_norm": 6.250141143798828,
      "learning_rate": 8.537117903930132e-06,
      "loss": 3.8582,
      "num_tokens": 8038.0,
      "step": 245
    },
    {
      "epoch": 0.17642907551164433,
      "grad_norm": 6.911887168884277,
      "learning_rate": 8.500727802037846e-06,
      "loss": 4.0152,
      "num_tokens": 8203.0,
      "step": 250
    },
    {
      "epoch": 0.17995765702187722,
      "grad_norm": 6.767223358154297,
      "learning_rate": 8.464337700145561e-06,
      "loss": 5.5789,
      "num_tokens": 8305.0,
      "step": 255
    },
    {
      "epoch": 0.1834862385321101,
      "grad_norm": 6.273744106292725,
      "learning_rate": 8.427947598253275e-06,
      "loss": 3.8701,
      "num_tokens": 8482.0,
      "step": 260
    },
    {
      "epoch": 0.187014820042343,
      "grad_norm": 7.748617649078369,
      "learning_rate": 8.39155749636099e-06,
      "loss": 4.2259,
      "num_tokens": 8594.0,
      "step": 265
    },
    {
      "epoch": 0.19054340155257588,
      "grad_norm": 5.387099742889404,
      "learning_rate": 8.355167394468706e-06,
      "loss": 4.0457,
      "num_tokens": 8755.0,
      "step": 270
    },
    {
      "epoch": 0.19407198306280876,
      "grad_norm": 11.409551620483398,
      "learning_rate": 8.31877729257642e-06,
      "loss": 4.4802,
      "num_tokens": 8899.0,
      "step": 275
    },
    {
      "epoch": 0.19760056457304165,
      "grad_norm": 6.489012718200684,
      "learning_rate": 8.282387190684135e-06,
      "loss": 4.1598,
      "num_tokens": 9067.0,
      "step": 280
    },
    {
      "epoch": 0.20112914608327453,
      "grad_norm": 6.242270469665527,
      "learning_rate": 8.245997088791849e-06,
      "loss": 3.8017,
      "num_tokens": 9220.0,
      "step": 285
    },
    {
      "epoch": 0.20465772759350742,
      "grad_norm": 8.354785919189453,
      "learning_rate": 8.209606986899564e-06,
      "loss": 3.4807,
      "num_tokens": 9408.0,
      "step": 290
    },
    {
      "epoch": 0.2081863091037403,
      "grad_norm": 7.369969367980957,
      "learning_rate": 8.17321688500728e-06,
      "loss": 4.0655,
      "num_tokens": 9593.0,
      "step": 295
    },
    {
      "epoch": 0.2117148906139732,
      "grad_norm": 5.379213809967041,
      "learning_rate": 8.136826783114993e-06,
      "loss": 3.6492,
      "num_tokens": 9846.0,
      "step": 300
    },
    {
      "epoch": 0.21524347212420608,
      "grad_norm": 5.986121654510498,
      "learning_rate": 8.100436681222707e-06,
      "loss": 4.3977,
      "num_tokens": 10002.0,
      "step": 305
    },
    {
      "epoch": 0.21877205363443897,
      "grad_norm": 7.945683479309082,
      "learning_rate": 8.064046579330422e-06,
      "loss": 3.863,
      "num_tokens": 10189.0,
      "step": 310
    },
    {
      "epoch": 0.22230063514467185,
      "grad_norm": 13.491762161254883,
      "learning_rate": 8.027656477438138e-06,
      "loss": 4.0785,
      "num_tokens": 10300.0,
      "step": 315
    },
    {
      "epoch": 0.22582921665490474,
      "grad_norm": 12.45767879486084,
      "learning_rate": 7.991266375545851e-06,
      "loss": 4.6192,
      "num_tokens": 10427.0,
      "step": 320
    },
    {
      "epoch": 0.22935779816513763,
      "grad_norm": 10.546112060546875,
      "learning_rate": 7.954876273653567e-06,
      "loss": 4.2873,
      "num_tokens": 10576.0,
      "step": 325
    },
    {
      "epoch": 0.23288637967537051,
      "grad_norm": 7.06276273727417,
      "learning_rate": 7.918486171761282e-06,
      "loss": 4.7017,
      "num_tokens": 10701.0,
      "step": 330
    },
    {
      "epoch": 0.2364149611856034,
      "grad_norm": 6.565213680267334,
      "learning_rate": 7.882096069868996e-06,
      "loss": 4.1675,
      "num_tokens": 10857.0,
      "step": 335
    },
    {
      "epoch": 0.2399435426958363,
      "grad_norm": 16.508094787597656,
      "learning_rate": 7.845705967976711e-06,
      "loss": 4.6825,
      "num_tokens": 10978.0,
      "step": 340
    },
    {
      "epoch": 0.24347212420606917,
      "grad_norm": 9.607285499572754,
      "learning_rate": 7.809315866084425e-06,
      "loss": 4.0037,
      "num_tokens": 11173.0,
      "step": 345
    },
    {
      "epoch": 0.24700070571630206,
      "grad_norm": 7.997066497802734,
      "learning_rate": 7.77292576419214e-06,
      "loss": 4.4387,
      "num_tokens": 11346.0,
      "step": 350
    },
    {
      "epoch": 0.25052928722653495,
      "grad_norm": 17.638559341430664,
      "learning_rate": 7.736535662299856e-06,
      "loss": 4.6185,
      "num_tokens": 11492.0,
      "step": 355
    },
    {
      "epoch": 0.25405786873676783,
      "grad_norm": 10.107475280761719,
      "learning_rate": 7.70014556040757e-06,
      "loss": 4.1789,
      "num_tokens": 11660.0,
      "step": 360
    },
    {
      "epoch": 0.2575864502470007,
      "grad_norm": 7.003572940826416,
      "learning_rate": 7.663755458515283e-06,
      "loss": 3.57,
      "num_tokens": 11828.0,
      "step": 365
    },
    {
      "epoch": 0.2611150317572336,
      "grad_norm": 16.055421829223633,
      "learning_rate": 7.627365356622999e-06,
      "loss": 4.3833,
      "num_tokens": 11973.0,
      "step": 370
    },
    {
      "epoch": 0.2646436132674665,
      "grad_norm": 9.768933296203613,
      "learning_rate": 7.590975254730713e-06,
      "loss": 3.8873,
      "num_tokens": 12137.0,
      "step": 375
    },
    {
      "epoch": 0.2681721947776994,
      "grad_norm": 15.694048881530762,
      "learning_rate": 7.554585152838429e-06,
      "loss": 4.2456,
      "num_tokens": 12301.0,
      "step": 380
    },
    {
      "epoch": 0.27170077628793227,
      "grad_norm": 5.737045764923096,
      "learning_rate": 7.518195050946143e-06,
      "loss": 3.4238,
      "num_tokens": 12488.0,
      "step": 385
    },
    {
      "epoch": 0.27522935779816515,
      "grad_norm": 6.535466194152832,
      "learning_rate": 7.481804949053858e-06,
      "loss": 3.8082,
      "num_tokens": 12671.0,
      "step": 390
    },
    {
      "epoch": 0.27875793930839804,
      "grad_norm": 5.947960376739502,
      "learning_rate": 7.4454148471615725e-06,
      "loss": 3.7361,
      "num_tokens": 12873.0,
      "step": 395
    },
    {
      "epoch": 0.2822865208186309,
      "grad_norm": 7.231363773345947,
      "learning_rate": 7.409024745269288e-06,
      "loss": 3.8535,
      "num_tokens": 13040.0,
      "step": 400
    },
    {
      "epoch": 0.2858151023288638,
      "grad_norm": 24.200801849365234,
      "learning_rate": 7.3726346433770024e-06,
      "loss": 4.6228,
      "num_tokens": 13214.0,
      "step": 405
    },
    {
      "epoch": 0.2893436838390967,
      "grad_norm": 8.3319673538208,
      "learning_rate": 7.336244541484717e-06,
      "loss": 3.9185,
      "num_tokens": 13382.0,
      "step": 410
    },
    {
      "epoch": 0.2928722653493296,
      "grad_norm": 9.764647483825684,
      "learning_rate": 7.299854439592432e-06,
      "loss": 4.2914,
      "num_tokens": 13536.0,
      "step": 415
    },
    {
      "epoch": 0.2964008468595625,
      "grad_norm": 16.503694534301758,
      "learning_rate": 7.263464337700145e-06,
      "loss": 4.1576,
      "num_tokens": 13707.0,
      "step": 420
    },
    {
      "epoch": 0.29992942836979536,
      "grad_norm": 10.765000343322754,
      "learning_rate": 7.227074235807861e-06,
      "loss": 4.6196,
      "num_tokens": 13839.0,
      "step": 425
    },
    {
      "epoch": 0.30345800988002825,
      "grad_norm": 9.557610511779785,
      "learning_rate": 7.190684133915575e-06,
      "loss": 4.0053,
      "num_tokens": 13991.0,
      "step": 430
    },
    {
      "epoch": 0.30698659139026113,
      "grad_norm": 8.862444877624512,
      "learning_rate": 7.15429403202329e-06,
      "loss": 3.6551,
      "num_tokens": 14218.0,
      "step": 435
    },
    {
      "epoch": 0.310515172900494,
      "grad_norm": 7.231827735900879,
      "learning_rate": 7.117903930131005e-06,
      "loss": 3.8795,
      "num_tokens": 14390.0,
      "step": 440
    },
    {
      "epoch": 0.3140437544107269,
      "grad_norm": 5.135258197784424,
      "learning_rate": 7.08151382823872e-06,
      "loss": 3.5203,
      "num_tokens": 14551.0,
      "step": 445
    },
    {
      "epoch": 0.3175723359209598,
      "grad_norm": 8.95759391784668,
      "learning_rate": 7.045123726346434e-06,
      "loss": 4.1815,
      "num_tokens": 14718.0,
      "step": 450
    },
    {
      "epoch": 0.3211009174311927,
      "grad_norm": 17.25215721130371,
      "learning_rate": 7.008733624454149e-06,
      "loss": 4.3459,
      "num_tokens": 14842.0,
      "step": 455
    },
    {
      "epoch": 0.32462949894142556,
      "grad_norm": 6.998918056488037,
      "learning_rate": 6.972343522561864e-06,
      "loss": 3.8097,
      "num_tokens": 15054.0,
      "step": 460
    },
    {
      "epoch": 0.32815808045165845,
      "grad_norm": 8.34299087524414,
      "learning_rate": 6.935953420669579e-06,
      "loss": 4.0311,
      "num_tokens": 15163.0,
      "step": 465
    },
    {
      "epoch": 0.33168666196189134,
      "grad_norm": 17.0415096282959,
      "learning_rate": 6.8995633187772934e-06,
      "loss": 3.7464,
      "num_tokens": 15331.0,
      "step": 470
    },
    {
      "epoch": 0.3352152434721242,
      "grad_norm": 8.620532035827637,
      "learning_rate": 6.863173216885007e-06,
      "loss": 3.9146,
      "num_tokens": 15476.0,
      "step": 475
    },
    {
      "epoch": 0.3387438249823571,
      "grad_norm": 10.483037948608398,
      "learning_rate": 6.8267831149927226e-06,
      "loss": 3.8817,
      "num_tokens": 15644.0,
      "step": 480
    },
    {
      "epoch": 0.34227240649259,
      "grad_norm": 8.426543235778809,
      "learning_rate": 6.790393013100437e-06,
      "loss": 4.1726,
      "num_tokens": 15807.0,
      "step": 485
    },
    {
      "epoch": 0.3458009880028229,
      "grad_norm": 10.764408111572266,
      "learning_rate": 6.754002911208152e-06,
      "loss": 4.3767,
      "num_tokens": 15969.0,
      "step": 490
    },
    {
      "epoch": 0.34932956951305577,
      "grad_norm": 16.056196212768555,
      "learning_rate": 6.717612809315866e-06,
      "loss": 4.0655,
      "num_tokens": 16146.0,
      "step": 495
    },
    {
      "epoch": 0.35285815102328866,
      "grad_norm": 15.497499465942383,
      "learning_rate": 6.681222707423582e-06,
      "loss": 4.1428,
      "num_tokens": 16289.0,
      "step": 500
    }
  ],
  "logging_steps": 5,
  "max_steps": 1417,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 354780072934560.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
