{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.7057163020465773,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0035285815102328866,
      "grad_norm": 25.921674728393555,
      "learning_rate": 9.30232558139535e-07,
      "loss": 10.2292,
      "num_tokens": 144.0,
      "step": 5
    },
    {
      "epoch": 0.007057163020465773,
      "grad_norm": 27.887800216674805,
      "learning_rate": 2.0930232558139536e-06,
      "loss": 8.5953,
      "num_tokens": 304.0,
      "step": 10
    },
    {
      "epoch": 0.01058574453069866,
      "grad_norm": 25.759153366088867,
      "learning_rate": 3.2558139534883724e-06,
      "loss": 9.1102,
      "num_tokens": 453.0,
      "step": 15
    },
    {
      "epoch": 0.014114326040931546,
      "grad_norm": 21.76809310913086,
      "learning_rate": 4.418604651162791e-06,
      "loss": 11.0589,
      "num_tokens": 577.0,
      "step": 20
    },
    {
      "epoch": 0.017642907551164433,
      "grad_norm": 17.176002502441406,
      "learning_rate": 5.58139534883721e-06,
      "loss": 8.1383,
      "num_tokens": 722.0,
      "step": 25
    },
    {
      "epoch": 0.02117148906139732,
      "grad_norm": 10.399138450622559,
      "learning_rate": 6.744186046511628e-06,
      "loss": 9.2162,
      "num_tokens": 861.0,
      "step": 30
    },
    {
      "epoch": 0.024700070571630206,
      "grad_norm": 13.782427787780762,
      "learning_rate": 7.906976744186048e-06,
      "loss": 8.3233,
      "num_tokens": 1027.0,
      "step": 35
    },
    {
      "epoch": 0.028228652081863093,
      "grad_norm": 12.461374282836914,
      "learning_rate": 9.069767441860465e-06,
      "loss": 7.7279,
      "num_tokens": 1191.0,
      "step": 40
    },
    {
      "epoch": 0.03175723359209598,
      "grad_norm": 4.2789530754089355,
      "learning_rate": 9.992721979621543e-06,
      "loss": 6.8317,
      "num_tokens": 1420.0,
      "step": 45
    },
    {
      "epoch": 0.035285815102328866,
      "grad_norm": 5.528181552886963,
      "learning_rate": 9.956331877729258e-06,
      "loss": 7.3611,
      "num_tokens": 1614.0,
      "step": 50
    },
    {
      "epoch": 0.03881439661256175,
      "grad_norm": 4.301918029785156,
      "learning_rate": 9.919941775836974e-06,
      "loss": 5.8643,
      "num_tokens": 1854.0,
      "step": 55
    },
    {
      "epoch": 0.04234297812279464,
      "grad_norm": 5.580268383026123,
      "learning_rate": 9.883551673944687e-06,
      "loss": 5.9467,
      "num_tokens": 2041.0,
      "step": 60
    },
    {
      "epoch": 0.045871559633027525,
      "grad_norm": 3.7024829387664795,
      "learning_rate": 9.847161572052403e-06,
      "loss": 7.7714,
      "num_tokens": 2217.0,
      "step": 65
    },
    {
      "epoch": 0.04940014114326041,
      "grad_norm": 8.496533393859863,
      "learning_rate": 9.810771470160118e-06,
      "loss": 6.629,
      "num_tokens": 2405.0,
      "step": 70
    },
    {
      "epoch": 0.0529287226534933,
      "grad_norm": 4.015966415405273,
      "learning_rate": 9.774381368267832e-06,
      "loss": 6.2269,
      "num_tokens": 2575.0,
      "step": 75
    },
    {
      "epoch": 0.056457304163726185,
      "grad_norm": 6.788255214691162,
      "learning_rate": 9.737991266375547e-06,
      "loss": 6.3991,
      "num_tokens": 2760.0,
      "step": 80
    },
    {
      "epoch": 0.05998588567395907,
      "grad_norm": 8.681844711303711,
      "learning_rate": 9.701601164483261e-06,
      "loss": 7.2611,
      "num_tokens": 2869.0,
      "step": 85
    },
    {
      "epoch": 0.06351446718419196,
      "grad_norm": 9.664346694946289,
      "learning_rate": 9.665211062590975e-06,
      "loss": 5.609,
      "num_tokens": 3039.0,
      "step": 90
    },
    {
      "epoch": 0.06704304869442484,
      "grad_norm": 5.559340953826904,
      "learning_rate": 9.62882096069869e-06,
      "loss": 6.7479,
      "num_tokens": 3198.0,
      "step": 95
    },
    {
      "epoch": 0.07057163020465773,
      "grad_norm": 13.2333402633667,
      "learning_rate": 9.592430858806406e-06,
      "loss": 5.5961,
      "num_tokens": 3376.0,
      "step": 100
    },
    {
      "epoch": 0.07410021171489062,
      "grad_norm": 6.381357192993164,
      "learning_rate": 9.55604075691412e-06,
      "loss": 4.8746,
      "num_tokens": 3511.0,
      "step": 105
    },
    {
      "epoch": 0.0776287932251235,
      "grad_norm": 7.3606109619140625,
      "learning_rate": 9.519650655021835e-06,
      "loss": 6.1467,
      "num_tokens": 3647.0,
      "step": 110
    },
    {
      "epoch": 0.08115737473535639,
      "grad_norm": 6.588961124420166,
      "learning_rate": 9.48326055312955e-06,
      "loss": 5.6679,
      "num_tokens": 3813.0,
      "step": 115
    },
    {
      "epoch": 0.08468595624558928,
      "grad_norm": 6.097724914550781,
      "learning_rate": 9.446870451237264e-06,
      "loss": 5.4035,
      "num_tokens": 3963.0,
      "step": 120
    },
    {
      "epoch": 0.08821453775582216,
      "grad_norm": 8.375110626220703,
      "learning_rate": 9.41048034934498e-06,
      "loss": 5.2554,
      "num_tokens": 4102.0,
      "step": 125
    },
    {
      "epoch": 0.09174311926605505,
      "grad_norm": 5.556113243103027,
      "learning_rate": 9.374090247452695e-06,
      "loss": 6.3747,
      "num_tokens": 4244.0,
      "step": 130
    },
    {
      "epoch": 0.09527170077628794,
      "grad_norm": 3.689164400100708,
      "learning_rate": 9.337700145560408e-06,
      "loss": 4.6173,
      "num_tokens": 4426.0,
      "step": 135
    },
    {
      "epoch": 0.09880028228652082,
      "grad_norm": 9.203723907470703,
      "learning_rate": 9.301310043668122e-06,
      "loss": 4.5421,
      "num_tokens": 4627.0,
      "step": 140
    },
    {
      "epoch": 0.10232886379675371,
      "grad_norm": 7.6088972091674805,
      "learning_rate": 9.264919941775838e-06,
      "loss": 5.4274,
      "num_tokens": 4742.0,
      "step": 145
    },
    {
      "epoch": 0.1058574453069866,
      "grad_norm": 7.335166931152344,
      "learning_rate": 9.228529839883553e-06,
      "loss": 4.8092,
      "num_tokens": 4886.0,
      "step": 150
    },
    {
      "epoch": 0.10938602681721948,
      "grad_norm": 11.899389266967773,
      "learning_rate": 9.192139737991267e-06,
      "loss": 5.5594,
      "num_tokens": 5021.0,
      "step": 155
    },
    {
      "epoch": 0.11291460832745237,
      "grad_norm": 10.671916961669922,
      "learning_rate": 9.155749636098982e-06,
      "loss": 4.9508,
      "num_tokens": 5160.0,
      "step": 160
    },
    {
      "epoch": 0.11644318983768526,
      "grad_norm": 5.434268951416016,
      "learning_rate": 9.119359534206696e-06,
      "loss": 4.714,
      "num_tokens": 5308.0,
      "step": 165
    },
    {
      "epoch": 0.11997177134791814,
      "grad_norm": 4.767143726348877,
      "learning_rate": 9.082969432314411e-06,
      "loss": 4.222,
      "num_tokens": 5490.0,
      "step": 170
    },
    {
      "epoch": 0.12350035285815103,
      "grad_norm": 11.47334098815918,
      "learning_rate": 9.046579330422127e-06,
      "loss": 4.7697,
      "num_tokens": 5675.0,
      "step": 175
    },
    {
      "epoch": 0.12702893436838392,
      "grad_norm": 5.585015773773193,
      "learning_rate": 9.01018922852984e-06,
      "loss": 4.571,
      "num_tokens": 5853.0,
      "step": 180
    },
    {
      "epoch": 0.1305575158786168,
      "grad_norm": 6.937180995941162,
      "learning_rate": 8.973799126637556e-06,
      "loss": 4.2559,
      "num_tokens": 6022.0,
      "step": 185
    },
    {
      "epoch": 0.1340860973888497,
      "grad_norm": 7.042027950286865,
      "learning_rate": 8.937409024745271e-06,
      "loss": 3.9259,
      "num_tokens": 6176.0,
      "step": 190
    },
    {
      "epoch": 0.13761467889908258,
      "grad_norm": 12.704859733581543,
      "learning_rate": 8.901018922852985e-06,
      "loss": 4.4998,
      "num_tokens": 6340.0,
      "step": 195
    },
    {
      "epoch": 0.14114326040931546,
      "grad_norm": 7.44657564163208,
      "learning_rate": 8.864628820960699e-06,
      "loss": 3.7444,
      "num_tokens": 6502.0,
      "step": 200
    },
    {
      "epoch": 0.14467184191954835,
      "grad_norm": 7.578185081481934,
      "learning_rate": 8.828238719068414e-06,
      "loss": 4.322,
      "num_tokens": 6695.0,
      "step": 205
    },
    {
      "epoch": 0.14820042342978124,
      "grad_norm": 11.32441520690918,
      "learning_rate": 8.79184861717613e-06,
      "loss": 5.0231,
      "num_tokens": 6796.0,
      "step": 210
    },
    {
      "epoch": 0.15172900494001412,
      "grad_norm": 9.793000221252441,
      "learning_rate": 8.755458515283843e-06,
      "loss": 3.9224,
      "num_tokens": 6954.0,
      "step": 215
    },
    {
      "epoch": 0.155257586450247,
      "grad_norm": 9.136043548583984,
      "learning_rate": 8.719068413391559e-06,
      "loss": 4.3296,
      "num_tokens": 7125.0,
      "step": 220
    },
    {
      "epoch": 0.1587861679604799,
      "grad_norm": 8.767952919006348,
      "learning_rate": 8.682678311499272e-06,
      "loss": 4.8129,
      "num_tokens": 7294.0,
      "step": 225
    },
    {
      "epoch": 0.16231474947071278,
      "grad_norm": 9.559598922729492,
      "learning_rate": 8.646288209606988e-06,
      "loss": 4.3172,
      "num_tokens": 7468.0,
      "step": 230
    },
    {
      "epoch": 0.16584333098094567,
      "grad_norm": 10.590702056884766,
      "learning_rate": 8.609898107714703e-06,
      "loss": 3.7625,
      "num_tokens": 7654.0,
      "step": 235
    },
    {
      "epoch": 0.16937191249117856,
      "grad_norm": 7.263049602508545,
      "learning_rate": 8.573508005822417e-06,
      "loss": 4.6005,
      "num_tokens": 7842.0,
      "step": 240
    },
    {
      "epoch": 0.17290049400141144,
      "grad_norm": 6.250141143798828,
      "learning_rate": 8.537117903930132e-06,
      "loss": 3.8582,
      "num_tokens": 8038.0,
      "step": 245
    },
    {
      "epoch": 0.17642907551164433,
      "grad_norm": 6.911887168884277,
      "learning_rate": 8.500727802037846e-06,
      "loss": 4.0152,
      "num_tokens": 8203.0,
      "step": 250
    },
    {
      "epoch": 0.17995765702187722,
      "grad_norm": 6.767223358154297,
      "learning_rate": 8.464337700145561e-06,
      "loss": 5.5789,
      "num_tokens": 8305.0,
      "step": 255
    },
    {
      "epoch": 0.1834862385321101,
      "grad_norm": 6.273744106292725,
      "learning_rate": 8.427947598253275e-06,
      "loss": 3.8701,
      "num_tokens": 8482.0,
      "step": 260
    },
    {
      "epoch": 0.187014820042343,
      "grad_norm": 7.748617649078369,
      "learning_rate": 8.39155749636099e-06,
      "loss": 4.2259,
      "num_tokens": 8594.0,
      "step": 265
    },
    {
      "epoch": 0.19054340155257588,
      "grad_norm": 5.387099742889404,
      "learning_rate": 8.355167394468706e-06,
      "loss": 4.0457,
      "num_tokens": 8755.0,
      "step": 270
    },
    {
      "epoch": 0.19407198306280876,
      "grad_norm": 11.409551620483398,
      "learning_rate": 8.31877729257642e-06,
      "loss": 4.4802,
      "num_tokens": 8899.0,
      "step": 275
    },
    {
      "epoch": 0.19760056457304165,
      "grad_norm": 6.489012718200684,
      "learning_rate": 8.282387190684135e-06,
      "loss": 4.1598,
      "num_tokens": 9067.0,
      "step": 280
    },
    {
      "epoch": 0.20112914608327453,
      "grad_norm": 6.242270469665527,
      "learning_rate": 8.245997088791849e-06,
      "loss": 3.8017,
      "num_tokens": 9220.0,
      "step": 285
    },
    {
      "epoch": 0.20465772759350742,
      "grad_norm": 8.354785919189453,
      "learning_rate": 8.209606986899564e-06,
      "loss": 3.4807,
      "num_tokens": 9408.0,
      "step": 290
    },
    {
      "epoch": 0.2081863091037403,
      "grad_norm": 7.369969367980957,
      "learning_rate": 8.17321688500728e-06,
      "loss": 4.0655,
      "num_tokens": 9593.0,
      "step": 295
    },
    {
      "epoch": 0.2117148906139732,
      "grad_norm": 5.379213809967041,
      "learning_rate": 8.136826783114993e-06,
      "loss": 3.6492,
      "num_tokens": 9846.0,
      "step": 300
    },
    {
      "epoch": 0.21524347212420608,
      "grad_norm": 5.986121654510498,
      "learning_rate": 8.100436681222707e-06,
      "loss": 4.3977,
      "num_tokens": 10002.0,
      "step": 305
    },
    {
      "epoch": 0.21877205363443897,
      "grad_norm": 7.945683479309082,
      "learning_rate": 8.064046579330422e-06,
      "loss": 3.863,
      "num_tokens": 10189.0,
      "step": 310
    },
    {
      "epoch": 0.22230063514467185,
      "grad_norm": 13.491762161254883,
      "learning_rate": 8.027656477438138e-06,
      "loss": 4.0785,
      "num_tokens": 10300.0,
      "step": 315
    },
    {
      "epoch": 0.22582921665490474,
      "grad_norm": 12.45767879486084,
      "learning_rate": 7.991266375545851e-06,
      "loss": 4.6192,
      "num_tokens": 10427.0,
      "step": 320
    },
    {
      "epoch": 0.22935779816513763,
      "grad_norm": 10.546112060546875,
      "learning_rate": 7.954876273653567e-06,
      "loss": 4.2873,
      "num_tokens": 10576.0,
      "step": 325
    },
    {
      "epoch": 0.23288637967537051,
      "grad_norm": 7.06276273727417,
      "learning_rate": 7.918486171761282e-06,
      "loss": 4.7017,
      "num_tokens": 10701.0,
      "step": 330
    },
    {
      "epoch": 0.2364149611856034,
      "grad_norm": 6.565213680267334,
      "learning_rate": 7.882096069868996e-06,
      "loss": 4.1675,
      "num_tokens": 10857.0,
      "step": 335
    },
    {
      "epoch": 0.2399435426958363,
      "grad_norm": 16.508094787597656,
      "learning_rate": 7.845705967976711e-06,
      "loss": 4.6825,
      "num_tokens": 10978.0,
      "step": 340
    },
    {
      "epoch": 0.24347212420606917,
      "grad_norm": 9.607285499572754,
      "learning_rate": 7.809315866084425e-06,
      "loss": 4.0037,
      "num_tokens": 11173.0,
      "step": 345
    },
    {
      "epoch": 0.24700070571630206,
      "grad_norm": 7.997066497802734,
      "learning_rate": 7.77292576419214e-06,
      "loss": 4.4387,
      "num_tokens": 11346.0,
      "step": 350
    },
    {
      "epoch": 0.25052928722653495,
      "grad_norm": 17.638559341430664,
      "learning_rate": 7.736535662299856e-06,
      "loss": 4.6185,
      "num_tokens": 11492.0,
      "step": 355
    },
    {
      "epoch": 0.25405786873676783,
      "grad_norm": 10.107475280761719,
      "learning_rate": 7.70014556040757e-06,
      "loss": 4.1789,
      "num_tokens": 11660.0,
      "step": 360
    },
    {
      "epoch": 0.2575864502470007,
      "grad_norm": 7.003572940826416,
      "learning_rate": 7.663755458515283e-06,
      "loss": 3.57,
      "num_tokens": 11828.0,
      "step": 365
    },
    {
      "epoch": 0.2611150317572336,
      "grad_norm": 16.055421829223633,
      "learning_rate": 7.627365356622999e-06,
      "loss": 4.3833,
      "num_tokens": 11973.0,
      "step": 370
    },
    {
      "epoch": 0.2646436132674665,
      "grad_norm": 9.768933296203613,
      "learning_rate": 7.590975254730713e-06,
      "loss": 3.8873,
      "num_tokens": 12137.0,
      "step": 375
    },
    {
      "epoch": 0.2681721947776994,
      "grad_norm": 15.694048881530762,
      "learning_rate": 7.554585152838429e-06,
      "loss": 4.2456,
      "num_tokens": 12301.0,
      "step": 380
    },
    {
      "epoch": 0.27170077628793227,
      "grad_norm": 5.737045764923096,
      "learning_rate": 7.518195050946143e-06,
      "loss": 3.4238,
      "num_tokens": 12488.0,
      "step": 385
    },
    {
      "epoch": 0.27522935779816515,
      "grad_norm": 6.535466194152832,
      "learning_rate": 7.481804949053858e-06,
      "loss": 3.8082,
      "num_tokens": 12671.0,
      "step": 390
    },
    {
      "epoch": 0.27875793930839804,
      "grad_norm": 5.947960376739502,
      "learning_rate": 7.4454148471615725e-06,
      "loss": 3.7361,
      "num_tokens": 12873.0,
      "step": 395
    },
    {
      "epoch": 0.2822865208186309,
      "grad_norm": 7.231363773345947,
      "learning_rate": 7.409024745269288e-06,
      "loss": 3.8535,
      "num_tokens": 13040.0,
      "step": 400
    },
    {
      "epoch": 0.2858151023288638,
      "grad_norm": 24.200801849365234,
      "learning_rate": 7.3726346433770024e-06,
      "loss": 4.6228,
      "num_tokens": 13214.0,
      "step": 405
    },
    {
      "epoch": 0.2893436838390967,
      "grad_norm": 8.3319673538208,
      "learning_rate": 7.336244541484717e-06,
      "loss": 3.9185,
      "num_tokens": 13382.0,
      "step": 410
    },
    {
      "epoch": 0.2928722653493296,
      "grad_norm": 9.764647483825684,
      "learning_rate": 7.299854439592432e-06,
      "loss": 4.2914,
      "num_tokens": 13536.0,
      "step": 415
    },
    {
      "epoch": 0.2964008468595625,
      "grad_norm": 16.503694534301758,
      "learning_rate": 7.263464337700145e-06,
      "loss": 4.1576,
      "num_tokens": 13707.0,
      "step": 420
    },
    {
      "epoch": 0.29992942836979536,
      "grad_norm": 10.765000343322754,
      "learning_rate": 7.227074235807861e-06,
      "loss": 4.6196,
      "num_tokens": 13839.0,
      "step": 425
    },
    {
      "epoch": 0.30345800988002825,
      "grad_norm": 9.557610511779785,
      "learning_rate": 7.190684133915575e-06,
      "loss": 4.0053,
      "num_tokens": 13991.0,
      "step": 430
    },
    {
      "epoch": 0.30698659139026113,
      "grad_norm": 8.862444877624512,
      "learning_rate": 7.15429403202329e-06,
      "loss": 3.6551,
      "num_tokens": 14218.0,
      "step": 435
    },
    {
      "epoch": 0.310515172900494,
      "grad_norm": 7.231827735900879,
      "learning_rate": 7.117903930131005e-06,
      "loss": 3.8795,
      "num_tokens": 14390.0,
      "step": 440
    },
    {
      "epoch": 0.3140437544107269,
      "grad_norm": 5.135258197784424,
      "learning_rate": 7.08151382823872e-06,
      "loss": 3.5203,
      "num_tokens": 14551.0,
      "step": 445
    },
    {
      "epoch": 0.3175723359209598,
      "grad_norm": 8.95759391784668,
      "learning_rate": 7.045123726346434e-06,
      "loss": 4.1815,
      "num_tokens": 14718.0,
      "step": 450
    },
    {
      "epoch": 0.3211009174311927,
      "grad_norm": 17.25215721130371,
      "learning_rate": 7.008733624454149e-06,
      "loss": 4.3459,
      "num_tokens": 14842.0,
      "step": 455
    },
    {
      "epoch": 0.32462949894142556,
      "grad_norm": 6.998918056488037,
      "learning_rate": 6.972343522561864e-06,
      "loss": 3.8097,
      "num_tokens": 15054.0,
      "step": 460
    },
    {
      "epoch": 0.32815808045165845,
      "grad_norm": 8.34299087524414,
      "learning_rate": 6.935953420669579e-06,
      "loss": 4.0311,
      "num_tokens": 15163.0,
      "step": 465
    },
    {
      "epoch": 0.33168666196189134,
      "grad_norm": 17.0415096282959,
      "learning_rate": 6.8995633187772934e-06,
      "loss": 3.7464,
      "num_tokens": 15331.0,
      "step": 470
    },
    {
      "epoch": 0.3352152434721242,
      "grad_norm": 8.620532035827637,
      "learning_rate": 6.863173216885007e-06,
      "loss": 3.9146,
      "num_tokens": 15476.0,
      "step": 475
    },
    {
      "epoch": 0.3387438249823571,
      "grad_norm": 10.483037948608398,
      "learning_rate": 6.8267831149927226e-06,
      "loss": 3.8817,
      "num_tokens": 15644.0,
      "step": 480
    },
    {
      "epoch": 0.34227240649259,
      "grad_norm": 8.426543235778809,
      "learning_rate": 6.790393013100437e-06,
      "loss": 4.1726,
      "num_tokens": 15807.0,
      "step": 485
    },
    {
      "epoch": 0.3458009880028229,
      "grad_norm": 10.764408111572266,
      "learning_rate": 6.754002911208152e-06,
      "loss": 4.3767,
      "num_tokens": 15969.0,
      "step": 490
    },
    {
      "epoch": 0.34932956951305577,
      "grad_norm": 16.056196212768555,
      "learning_rate": 6.717612809315866e-06,
      "loss": 4.0655,
      "num_tokens": 16146.0,
      "step": 495
    },
    {
      "epoch": 0.35285815102328866,
      "grad_norm": 15.497499465942383,
      "learning_rate": 6.681222707423582e-06,
      "loss": 4.1428,
      "num_tokens": 16289.0,
      "step": 500
    },
    {
      "epoch": 0.35638673253352154,
      "grad_norm": 7.897292613983154,
      "learning_rate": 6.644832605531296e-06,
      "loss": 3.8255,
      "num_tokens": 16486.0,
      "step": 505
    },
    {
      "epoch": 0.35991531404375443,
      "grad_norm": 11.063811302185059,
      "learning_rate": 6.608442503639011e-06,
      "loss": 3.8935,
      "num_tokens": 16683.0,
      "step": 510
    },
    {
      "epoch": 0.3634438955539873,
      "grad_norm": 20.72017478942871,
      "learning_rate": 6.572052401746725e-06,
      "loss": 3.6833,
      "num_tokens": 16867.0,
      "step": 515
    },
    {
      "epoch": 0.3669724770642202,
      "grad_norm": 6.198633670806885,
      "learning_rate": 6.535662299854441e-06,
      "loss": 3.5752,
      "num_tokens": 17064.0,
      "step": 520
    },
    {
      "epoch": 0.3705010585744531,
      "grad_norm": 13.658638000488281,
      "learning_rate": 6.499272197962155e-06,
      "loss": 4.1055,
      "num_tokens": 17264.0,
      "step": 525
    },
    {
      "epoch": 0.374029640084686,
      "grad_norm": 11.58011245727539,
      "learning_rate": 6.462882096069869e-06,
      "loss": 4.6093,
      "num_tokens": 17378.0,
      "step": 530
    },
    {
      "epoch": 0.37755822159491886,
      "grad_norm": 6.4659953117370605,
      "learning_rate": 6.426491994177584e-06,
      "loss": 3.7823,
      "num_tokens": 17547.0,
      "step": 535
    },
    {
      "epoch": 0.38108680310515175,
      "grad_norm": 10.828516006469727,
      "learning_rate": 6.390101892285299e-06,
      "loss": 4.0081,
      "num_tokens": 17699.0,
      "step": 540
    },
    {
      "epoch": 0.38461538461538464,
      "grad_norm": 18.61933135986328,
      "learning_rate": 6.353711790393014e-06,
      "loss": 4.2197,
      "num_tokens": 17851.0,
      "step": 545
    },
    {
      "epoch": 0.3881439661256175,
      "grad_norm": 7.249997138977051,
      "learning_rate": 6.317321688500728e-06,
      "loss": 3.7676,
      "num_tokens": 18008.0,
      "step": 550
    },
    {
      "epoch": 0.3916725476358504,
      "grad_norm": 9.986129760742188,
      "learning_rate": 6.280931586608443e-06,
      "loss": 3.7749,
      "num_tokens": 18166.0,
      "step": 555
    },
    {
      "epoch": 0.3952011291460833,
      "grad_norm": 7.549842834472656,
      "learning_rate": 6.244541484716158e-06,
      "loss": 3.6474,
      "num_tokens": 18357.0,
      "step": 560
    },
    {
      "epoch": 0.3987297106563162,
      "grad_norm": 10.693854331970215,
      "learning_rate": 6.208151382823873e-06,
      "loss": 3.5975,
      "num_tokens": 18523.0,
      "step": 565
    },
    {
      "epoch": 0.40225829216654907,
      "grad_norm": 7.187129974365234,
      "learning_rate": 6.171761280931587e-06,
      "loss": 3.3349,
      "num_tokens": 18742.0,
      "step": 570
    },
    {
      "epoch": 0.40578687367678196,
      "grad_norm": 7.296199798583984,
      "learning_rate": 6.135371179039303e-06,
      "loss": 3.7199,
      "num_tokens": 18935.0,
      "step": 575
    },
    {
      "epoch": 0.40931545518701484,
      "grad_norm": 9.155189514160156,
      "learning_rate": 6.098981077147017e-06,
      "loss": 4.0155,
      "num_tokens": 19097.0,
      "step": 580
    },
    {
      "epoch": 0.41284403669724773,
      "grad_norm": 15.50386905670166,
      "learning_rate": 6.062590975254731e-06,
      "loss": 3.4465,
      "num_tokens": 19251.0,
      "step": 585
    },
    {
      "epoch": 0.4163726182074806,
      "grad_norm": 7.226724624633789,
      "learning_rate": 6.0262008733624455e-06,
      "loss": 3.2809,
      "num_tokens": 19417.0,
      "step": 590
    },
    {
      "epoch": 0.4199011997177135,
      "grad_norm": 8.522372245788574,
      "learning_rate": 5.98981077147016e-06,
      "loss": 3.8378,
      "num_tokens": 19626.0,
      "step": 595
    },
    {
      "epoch": 0.4234297812279464,
      "grad_norm": 6.31503438949585,
      "learning_rate": 5.9534206695778755e-06,
      "loss": 4.1962,
      "num_tokens": 19786.0,
      "step": 600
    },
    {
      "epoch": 0.4269583627381793,
      "grad_norm": 9.543560981750488,
      "learning_rate": 5.91703056768559e-06,
      "loss": 4.0739,
      "num_tokens": 19938.0,
      "step": 605
    },
    {
      "epoch": 0.43048694424841216,
      "grad_norm": 12.68901252746582,
      "learning_rate": 5.880640465793305e-06,
      "loss": 4.5471,
      "num_tokens": 20045.0,
      "step": 610
    },
    {
      "epoch": 0.43401552575864505,
      "grad_norm": 13.668021202087402,
      "learning_rate": 5.844250363901019e-06,
      "loss": 4.0214,
      "num_tokens": 20214.0,
      "step": 615
    },
    {
      "epoch": 0.43754410726887794,
      "grad_norm": 7.547858715057373,
      "learning_rate": 5.8078602620087346e-06,
      "loss": 3.468,
      "num_tokens": 20415.0,
      "step": 620
    },
    {
      "epoch": 0.4410726887791108,
      "grad_norm": 8.780203819274902,
      "learning_rate": 5.771470160116449e-06,
      "loss": 4.209,
      "num_tokens": 20566.0,
      "step": 625
    },
    {
      "epoch": 0.4446012702893437,
      "grad_norm": 7.061577320098877,
      "learning_rate": 5.735080058224164e-06,
      "loss": 3.659,
      "num_tokens": 20733.0,
      "step": 630
    },
    {
      "epoch": 0.4481298517995766,
      "grad_norm": 14.848505973815918,
      "learning_rate": 5.698689956331879e-06,
      "loss": 3.8357,
      "num_tokens": 20921.0,
      "step": 635
    },
    {
      "epoch": 0.4516584333098095,
      "grad_norm": 11.949190139770508,
      "learning_rate": 5.662299854439593e-06,
      "loss": 4.4076,
      "num_tokens": 21062.0,
      "step": 640
    },
    {
      "epoch": 0.45518701482004237,
      "grad_norm": 13.45275592803955,
      "learning_rate": 5.625909752547307e-06,
      "loss": 4.2122,
      "num_tokens": 21222.0,
      "step": 645
    },
    {
      "epoch": 0.45871559633027525,
      "grad_norm": 12.231271743774414,
      "learning_rate": 5.589519650655022e-06,
      "loss": 3.7785,
      "num_tokens": 21397.0,
      "step": 650
    },
    {
      "epoch": 0.46224417784050814,
      "grad_norm": 7.9544148445129395,
      "learning_rate": 5.5531295487627365e-06,
      "loss": 3.7079,
      "num_tokens": 21599.0,
      "step": 655
    },
    {
      "epoch": 0.46577275935074103,
      "grad_norm": 13.939010620117188,
      "learning_rate": 5.516739446870452e-06,
      "loss": 3.6243,
      "num_tokens": 21826.0,
      "step": 660
    },
    {
      "epoch": 0.4693013408609739,
      "grad_norm": 9.764778137207031,
      "learning_rate": 5.4803493449781665e-06,
      "loss": 3.5264,
      "num_tokens": 22002.0,
      "step": 665
    },
    {
      "epoch": 0.4728299223712068,
      "grad_norm": 8.495096206665039,
      "learning_rate": 5.443959243085881e-06,
      "loss": 3.7808,
      "num_tokens": 22165.0,
      "step": 670
    },
    {
      "epoch": 0.4763585038814397,
      "grad_norm": 15.898691177368164,
      "learning_rate": 5.407569141193596e-06,
      "loss": 4.7058,
      "num_tokens": 22275.0,
      "step": 675
    },
    {
      "epoch": 0.4798870853916726,
      "grad_norm": 14.654023170471191,
      "learning_rate": 5.371179039301311e-06,
      "loss": 4.157,
      "num_tokens": 22433.0,
      "step": 680
    },
    {
      "epoch": 0.48341566690190546,
      "grad_norm": 8.409130096435547,
      "learning_rate": 5.334788937409026e-06,
      "loss": 3.9757,
      "num_tokens": 22554.0,
      "step": 685
    },
    {
      "epoch": 0.48694424841213835,
      "grad_norm": 13.55532455444336,
      "learning_rate": 5.29839883551674e-06,
      "loss": 4.0149,
      "num_tokens": 22725.0,
      "step": 690
    },
    {
      "epoch": 0.49047282992237123,
      "grad_norm": 9.91893196105957,
      "learning_rate": 5.262008733624454e-06,
      "loss": 3.4942,
      "num_tokens": 22909.0,
      "step": 695
    },
    {
      "epoch": 0.4940014114326041,
      "grad_norm": 7.745805740356445,
      "learning_rate": 5.225618631732169e-06,
      "loss": 3.7575,
      "num_tokens": 23104.0,
      "step": 700
    },
    {
      "epoch": 0.497529992942837,
      "grad_norm": 10.257494926452637,
      "learning_rate": 5.189228529839884e-06,
      "loss": 3.6992,
      "num_tokens": 23273.0,
      "step": 705
    },
    {
      "epoch": 0.5010585744530699,
      "grad_norm": 10.817573547363281,
      "learning_rate": 5.152838427947598e-06,
      "loss": 3.8313,
      "num_tokens": 23383.0,
      "step": 710
    },
    {
      "epoch": 0.5045871559633027,
      "grad_norm": 9.4981689453125,
      "learning_rate": 5.116448326055313e-06,
      "loss": 4.2769,
      "num_tokens": 23513.0,
      "step": 715
    },
    {
      "epoch": 0.5081157374735357,
      "grad_norm": 10.784070014953613,
      "learning_rate": 5.080058224163028e-06,
      "loss": 4.4118,
      "num_tokens": 23637.0,
      "step": 720
    },
    {
      "epoch": 0.5116443189837685,
      "grad_norm": 5.929801940917969,
      "learning_rate": 5.043668122270743e-06,
      "loss": 3.8602,
      "num_tokens": 23820.0,
      "step": 725
    },
    {
      "epoch": 0.5151729004940014,
      "grad_norm": 13.104647636413574,
      "learning_rate": 5.0072780203784575e-06,
      "loss": 4.3237,
      "num_tokens": 23983.0,
      "step": 730
    },
    {
      "epoch": 0.5187014820042343,
      "grad_norm": 12.710762023925781,
      "learning_rate": 4.970887918486172e-06,
      "loss": 4.3934,
      "num_tokens": 24103.0,
      "step": 735
    },
    {
      "epoch": 0.5222300635144672,
      "grad_norm": 8.688497543334961,
      "learning_rate": 4.934497816593887e-06,
      "loss": 3.7826,
      "num_tokens": 24295.0,
      "step": 740
    },
    {
      "epoch": 0.5257586450247,
      "grad_norm": 13.548182487487793,
      "learning_rate": 4.898107714701601e-06,
      "loss": 3.5761,
      "num_tokens": 24513.0,
      "step": 745
    },
    {
      "epoch": 0.529287226534933,
      "grad_norm": 7.9744133949279785,
      "learning_rate": 4.861717612809317e-06,
      "loss": 4.2396,
      "num_tokens": 24652.0,
      "step": 750
    },
    {
      "epoch": 0.5328158080451658,
      "grad_norm": 15.582201957702637,
      "learning_rate": 4.825327510917031e-06,
      "loss": 4.1695,
      "num_tokens": 24751.0,
      "step": 755
    },
    {
      "epoch": 0.5363443895553988,
      "grad_norm": 14.550298690795898,
      "learning_rate": 4.788937409024746e-06,
      "loss": 3.8711,
      "num_tokens": 24884.0,
      "step": 760
    },
    {
      "epoch": 0.5398729710656316,
      "grad_norm": 13.491508483886719,
      "learning_rate": 4.75254730713246e-06,
      "loss": 3.8016,
      "num_tokens": 25045.0,
      "step": 765
    },
    {
      "epoch": 0.5434015525758645,
      "grad_norm": 10.895832061767578,
      "learning_rate": 4.716157205240175e-06,
      "loss": 3.6363,
      "num_tokens": 25243.0,
      "step": 770
    },
    {
      "epoch": 0.5469301340860974,
      "grad_norm": 8.971349716186523,
      "learning_rate": 4.679767103347889e-06,
      "loss": 4.1872,
      "num_tokens": 25378.0,
      "step": 775
    },
    {
      "epoch": 0.5504587155963303,
      "grad_norm": 10.91881275177002,
      "learning_rate": 4.643377001455605e-06,
      "loss": 4.1937,
      "num_tokens": 25512.0,
      "step": 780
    },
    {
      "epoch": 0.5539872971065631,
      "grad_norm": 8.583771705627441,
      "learning_rate": 4.606986899563319e-06,
      "loss": 3.6641,
      "num_tokens": 25719.0,
      "step": 785
    },
    {
      "epoch": 0.5575158786167961,
      "grad_norm": 8.883675575256348,
      "learning_rate": 4.570596797671034e-06,
      "loss": 3.7839,
      "num_tokens": 25865.0,
      "step": 790
    },
    {
      "epoch": 0.5610444601270289,
      "grad_norm": 16.489133834838867,
      "learning_rate": 4.5342066957787485e-06,
      "loss": 3.6159,
      "num_tokens": 26036.0,
      "step": 795
    },
    {
      "epoch": 0.5645730416372619,
      "grad_norm": 8.69708251953125,
      "learning_rate": 4.497816593886463e-06,
      "loss": 3.3944,
      "num_tokens": 26172.0,
      "step": 800
    },
    {
      "epoch": 0.5681016231474947,
      "grad_norm": 17.280927658081055,
      "learning_rate": 4.461426491994178e-06,
      "loss": 3.9275,
      "num_tokens": 26330.0,
      "step": 805
    },
    {
      "epoch": 0.5716302046577276,
      "grad_norm": 8.72150993347168,
      "learning_rate": 4.425036390101893e-06,
      "loss": 3.5707,
      "num_tokens": 26524.0,
      "step": 810
    },
    {
      "epoch": 0.5751587861679605,
      "grad_norm": 8.38675594329834,
      "learning_rate": 4.388646288209608e-06,
      "loss": 3.6993,
      "num_tokens": 26702.0,
      "step": 815
    },
    {
      "epoch": 0.5786873676781934,
      "grad_norm": 11.432948112487793,
      "learning_rate": 4.352256186317322e-06,
      "loss": 3.4914,
      "num_tokens": 26857.0,
      "step": 820
    },
    {
      "epoch": 0.5822159491884262,
      "grad_norm": 8.26421070098877,
      "learning_rate": 4.315866084425037e-06,
      "loss": 3.6521,
      "num_tokens": 27009.0,
      "step": 825
    },
    {
      "epoch": 0.5857445306986592,
      "grad_norm": 7.553324222564697,
      "learning_rate": 4.279475982532751e-06,
      "loss": 3.6514,
      "num_tokens": 27204.0,
      "step": 830
    },
    {
      "epoch": 0.589273112208892,
      "grad_norm": 14.210834503173828,
      "learning_rate": 4.243085880640466e-06,
      "loss": 3.9553,
      "num_tokens": 27364.0,
      "step": 835
    },
    {
      "epoch": 0.592801693719125,
      "grad_norm": 7.192838191986084,
      "learning_rate": 4.206695778748181e-06,
      "loss": 4.0687,
      "num_tokens": 27504.0,
      "step": 840
    },
    {
      "epoch": 0.5963302752293578,
      "grad_norm": 13.32080078125,
      "learning_rate": 4.170305676855895e-06,
      "loss": 4.0793,
      "num_tokens": 27637.0,
      "step": 845
    },
    {
      "epoch": 0.5998588567395907,
      "grad_norm": 11.145111083984375,
      "learning_rate": 4.13391557496361e-06,
      "loss": 3.9916,
      "num_tokens": 27805.0,
      "step": 850
    },
    {
      "epoch": 0.6033874382498235,
      "grad_norm": 10.712839126586914,
      "learning_rate": 4.097525473071325e-06,
      "loss": 3.856,
      "num_tokens": 27944.0,
      "step": 855
    },
    {
      "epoch": 0.6069160197600565,
      "grad_norm": 10.28277587890625,
      "learning_rate": 4.0611353711790395e-06,
      "loss": 3.911,
      "num_tokens": 28069.0,
      "step": 860
    },
    {
      "epoch": 0.6104446012702893,
      "grad_norm": 9.30447769165039,
      "learning_rate": 4.024745269286754e-06,
      "loss": 3.3358,
      "num_tokens": 28260.0,
      "step": 865
    },
    {
      "epoch": 0.6139731827805223,
      "grad_norm": 10.768399238586426,
      "learning_rate": 3.9883551673944695e-06,
      "loss": 3.9736,
      "num_tokens": 28427.0,
      "step": 870
    },
    {
      "epoch": 0.6175017642907551,
      "grad_norm": 7.874086856842041,
      "learning_rate": 3.951965065502183e-06,
      "loss": 3.7236,
      "num_tokens": 28645.0,
      "step": 875
    },
    {
      "epoch": 0.621030345800988,
      "grad_norm": 7.762902736663818,
      "learning_rate": 3.915574963609899e-06,
      "loss": 3.7401,
      "num_tokens": 28794.0,
      "step": 880
    },
    {
      "epoch": 0.6245589273112209,
      "grad_norm": 11.626106262207031,
      "learning_rate": 3.879184861717613e-06,
      "loss": 3.923,
      "num_tokens": 28939.0,
      "step": 885
    },
    {
      "epoch": 0.6280875088214538,
      "grad_norm": 19.405006408691406,
      "learning_rate": 3.842794759825328e-06,
      "loss": 4.4998,
      "num_tokens": 29067.0,
      "step": 890
    },
    {
      "epoch": 0.6316160903316866,
      "grad_norm": 9.863045692443848,
      "learning_rate": 3.8064046579330427e-06,
      "loss": 3.7424,
      "num_tokens": 29231.0,
      "step": 895
    },
    {
      "epoch": 0.6351446718419196,
      "grad_norm": 7.8835015296936035,
      "learning_rate": 3.770014556040757e-06,
      "loss": 4.0902,
      "num_tokens": 29379.0,
      "step": 900
    },
    {
      "epoch": 0.6386732533521524,
      "grad_norm": 17.492860794067383,
      "learning_rate": 3.733624454148472e-06,
      "loss": 4.1686,
      "num_tokens": 29479.0,
      "step": 905
    },
    {
      "epoch": 0.6422018348623854,
      "grad_norm": 15.813787460327148,
      "learning_rate": 3.6972343522561864e-06,
      "loss": 4.1452,
      "num_tokens": 29662.0,
      "step": 910
    },
    {
      "epoch": 0.6457304163726182,
      "grad_norm": 9.879055976867676,
      "learning_rate": 3.6608442503639014e-06,
      "loss": 3.8432,
      "num_tokens": 29828.0,
      "step": 915
    },
    {
      "epoch": 0.6492589978828511,
      "grad_norm": 11.680241584777832,
      "learning_rate": 3.624454148471616e-06,
      "loss": 3.3997,
      "num_tokens": 30023.0,
      "step": 920
    },
    {
      "epoch": 0.652787579393084,
      "grad_norm": 11.332480430603027,
      "learning_rate": 3.588064046579331e-06,
      "loss": 3.3339,
      "num_tokens": 30224.0,
      "step": 925
    },
    {
      "epoch": 0.6563161609033169,
      "grad_norm": 12.511892318725586,
      "learning_rate": 3.551673944687045e-06,
      "loss": 3.4919,
      "num_tokens": 30432.0,
      "step": 930
    },
    {
      "epoch": 0.6598447424135497,
      "grad_norm": 10.686097145080566,
      "learning_rate": 3.51528384279476e-06,
      "loss": 3.3892,
      "num_tokens": 30605.0,
      "step": 935
    },
    {
      "epoch": 0.6633733239237827,
      "grad_norm": 18.294973373413086,
      "learning_rate": 3.4788937409024746e-06,
      "loss": 4.3567,
      "num_tokens": 30718.0,
      "step": 940
    },
    {
      "epoch": 0.6669019054340155,
      "grad_norm": 13.539057731628418,
      "learning_rate": 3.4425036390101896e-06,
      "loss": 3.3713,
      "num_tokens": 30920.0,
      "step": 945
    },
    {
      "epoch": 0.6704304869442484,
      "grad_norm": 9.639989852905273,
      "learning_rate": 3.406113537117904e-06,
      "loss": 3.4836,
      "num_tokens": 31110.0,
      "step": 950
    },
    {
      "epoch": 0.6739590684544813,
      "grad_norm": 14.067679405212402,
      "learning_rate": 3.369723435225619e-06,
      "loss": 3.9813,
      "num_tokens": 31294.0,
      "step": 955
    },
    {
      "epoch": 0.6774876499647142,
      "grad_norm": 10.160487174987793,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 3.9905,
      "num_tokens": 31421.0,
      "step": 960
    },
    {
      "epoch": 0.681016231474947,
      "grad_norm": 19.487062454223633,
      "learning_rate": 3.2969432314410483e-06,
      "loss": 3.5594,
      "num_tokens": 31564.0,
      "step": 965
    },
    {
      "epoch": 0.68454481298518,
      "grad_norm": 10.001561164855957,
      "learning_rate": 3.260553129548763e-06,
      "loss": 3.5474,
      "num_tokens": 31707.0,
      "step": 970
    },
    {
      "epoch": 0.6880733944954128,
      "grad_norm": 8.872093200683594,
      "learning_rate": 3.224163027656478e-06,
      "loss": 3.4299,
      "num_tokens": 31884.0,
      "step": 975
    },
    {
      "epoch": 0.6916019760056458,
      "grad_norm": 17.75981903076172,
      "learning_rate": 3.1877729257641924e-06,
      "loss": 3.8037,
      "num_tokens": 32038.0,
      "step": 980
    },
    {
      "epoch": 0.6951305575158786,
      "grad_norm": 14.286235809326172,
      "learning_rate": 3.151382823871907e-06,
      "loss": 4.1348,
      "num_tokens": 32154.0,
      "step": 985
    },
    {
      "epoch": 0.6986591390261115,
      "grad_norm": 10.528822898864746,
      "learning_rate": 3.1149927219796215e-06,
      "loss": 3.584,
      "num_tokens": 32335.0,
      "step": 990
    },
    {
      "epoch": 0.7021877205363444,
      "grad_norm": 6.78510856628418,
      "learning_rate": 3.0786026200873365e-06,
      "loss": 3.7375,
      "num_tokens": 32534.0,
      "step": 995
    },
    {
      "epoch": 0.7057163020465773,
      "grad_norm": 13.580316543579102,
      "learning_rate": 3.042212518195051e-06,
      "loss": 4.7532,
      "num_tokens": 32664.0,
      "step": 1000
    }
  ],
  "logging_steps": 5,
  "max_steps": 1417,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 711433255714560.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
